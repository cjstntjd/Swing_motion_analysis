{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.naive_bayes as nb\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import LSTM,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout,Input,Dense,Activation,Flatten,SeparableConv2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### random_forest model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this model is  RandomForest \n",
      "\n",
      "\n",
      "load_file_complete\n",
      "make DataFrame...\n",
      "make DataFrame complete ....\n",
      "label_encoding processing...\n",
      "model best parameter :  {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}\n",
      "fitting  RandomForest  is complete...\n",
      "RandomForest score is : 0.9767981438515081\n",
      "Validation Result...\n",
      "hit:  842  miss :  20\n",
      "\n",
      "\n",
      "now test new data..\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "back_smash  -->  back_cut err_index :  73\n",
      "back_smash  -->  back_cut err_index :  74\n",
      "back_smash  -->  back_cut err_index :  75\n",
      "back_smash  -->  back_cut err_index :  77\n",
      "back_smash  -->  back_cut err_index :  84\n",
      "back_smash  -->  back_cut err_index :  85\n",
      "back_smash  -->  fo_smash err_index :  86\n",
      "hit:  80  miss :  7\n"
     ]
    }
   ],
   "source": [
    "target_folder = 'swing'\n",
    "mode = 'RandomForest'\n",
    "\n",
    "print('this model is ',mode,'\\n\\n')\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk(target_folder):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "print('load_file_complete')\n",
    "print('make DataFrame...')\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "print('make DataFrame complete ....')\n",
    "\n",
    "print('label_encoding processing...')\n",
    "encoder = LabelEncoder()\n",
    "arr_label = encoder.fit_transform(total_label)\n",
    "\n",
    "total_data = np.array(total_data)\n",
    "arr_label = np.array(arr_label)\n",
    "x_train,x_test,y_train,y_test = train_test_split(total_data,arr_label,test_size=0.2,random_state=0)\n",
    "\n",
    "nsamples, nx, ny = x_train.shape\n",
    "d2_train_dataset = x_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = x_test.shape\n",
    "d2_test_dataset = x_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "\n",
    "#####here to change####\n",
    "######################################################################\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "param_grid = { \n",
    "    'n_estimators': [10,15,20,30,40,50,100,200,300,500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [3,4,5,6,7,8,9,10],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "mod = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10,n_jobs=-1)\n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('model best parameter : ',predict_model.best_params_)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SVM model predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this model is  SVM \n",
      "\n",
      "\n",
      "load_file_complete\n",
      "make DataFrame...\n",
      "make DataFrame complete ....\n",
      "label_encoding processing...\n",
      "best_parameter is change :  {'kernel': 'linear', 'gamma': 0.001, 'C': 0.001}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.001}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.001}\n",
      "best_parameter is change :  {'kernel': 'linear', 'gamma': 0.001, 'C': 0.01}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.01}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.01}\n",
      "best_parameter is change :  {'kernel': 'linear', 'gamma': 0.001, 'C': 0.1}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.1}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.1}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.1}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.1}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.1}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.1}\n",
      "best_parameter is change :  {'kernel': 'rbf', 'gamma': 0.001, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 10}\n",
      "best_parameter is change :  {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.001, 'C': 100}\n",
      "best_parameter is change :  {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "remain : {'kernel': 'rbf', 'gamma': 0.01, 'C': 10}\n",
      "fitting  SVM  is complete...\n",
      "SVM score is : 0.9817131857555341\n",
      "Validation Result...\n",
      "hit:  1020  miss :  19\n",
      "\n",
      "\n",
      "now test new data..\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "back_smash  -->  fo_smash err_index :  84\n",
      "hit:  86  miss :  1\n"
     ]
    }
   ],
   "source": [
    "target_folder = 'swing'\n",
    "mode = 'SVM'\n",
    "#############################################################\n",
    "\n",
    "print('this model is ',mode,'\\n\\n')\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk(target_folder):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "print('load_file_complete')\n",
    "print('make DataFrame...')\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "print('make DataFrame complete ....')\n",
    "\n",
    "print('label_encoding processing...')\n",
    "encoder = LabelEncoder()\n",
    "arr_label = encoder.fit_transform(total_label)\n",
    "\n",
    "total_data = np.array(total_data)\n",
    "arr_label = np.array(arr_label)\n",
    "x_train,x_test,y_train,y_test = train_test_split(total_data,arr_label,test_size=0.2,random_state=0)\n",
    "\n",
    "nsamples, nx, ny = x_train.shape\n",
    "d2_train_dataset = x_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = x_test.shape\n",
    "d2_test_dataset = x_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "\n",
    "#####here to change####\n",
    "######################################################################\n",
    "\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for gamma in [0.001,0.01,0.1,1,10,100]:\n",
    "    for C in [0.001,0.01,0.1,1,10,100]:\n",
    "        for kernel in ['linear','rbf','poly']:\n",
    "            tmp_model = svm.SVC(kernel=kernel,gamma=gamma,C=C)\n",
    "            scores = cross_val_score(tmp_model,d2_train_dataset,y_train,cv=10,n_jobs=-1)\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if score>best_score:\n",
    "                \n",
    "                best_score = score\n",
    "                best_parameter = {'kernel':kernel,'gamma':gamma,'C':C}\n",
    "                print('best_parameter is change : ',best_parameter)\n",
    "            else:\n",
    "                print('remain :',best_parameter)\n",
    "mod = svm.SVC(**best_parameter)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Naive Bayes model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this model is  Naive_Bayes \n",
      "\n",
      "\n",
      "load_file_complete\n",
      "make DataFrame...\n",
      "make DataFrame complete ....\n",
      "label_encoding processing...\n",
      "fitting  Naive_Bayes  is complete...\n",
      "Naive_Bayes score is : 0.888631090487239\n",
      "Validation Result...\n",
      "hit:  766  miss :  96\n",
      "\n",
      "\n",
      "now test new data..\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "back_smash  -->  fo_drive err_index :  73\n",
      "back_smash  -->  fo_drive err_index :  74\n",
      "back_smash  -->  fo_drive err_index :  75\n",
      "back_smash  -->  fo_drive err_index :  77\n",
      "back_smash  -->  fo_drive err_index :  84\n",
      "back_smash  -->  fo_drive err_index :  85\n",
      "back_smash  -->  fo_drive err_index :  86\n",
      "hit:  80  miss :  7\n"
     ]
    }
   ],
   "source": [
    "target_folder = 'swing'\n",
    "mode = 'Naive_Bayes'\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "print('this model is ',mode,'\\n\\n')\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk(target_folder):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "print('load_file_complete')\n",
    "print('make DataFrame...')\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "print('make DataFrame complete ....')\n",
    "\n",
    "print('label_encoding processing...')\n",
    "encoder = LabelEncoder()\n",
    "arr_label = encoder.fit_transform(total_label)\n",
    "\n",
    "total_data = np.array(total_data)\n",
    "arr_label = np.array(arr_label)\n",
    "x_train,x_test,y_train,y_test = train_test_split(total_data,arr_label,test_size=0.2,random_state=0)\n",
    "\n",
    "nsamples, nx, ny = x_train.shape\n",
    "d2_train_dataset = x_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = x_test.shape\n",
    "d2_test_dataset = x_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "\n",
    "#####here to change####\n",
    "######################################################################\n",
    "\n",
    "\n",
    "mod = GaussianNB()\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############XG boost#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this model is  XGBoost Classifier \n",
      "\n",
      "\n",
      "load_file_complete\n",
      "make DataFrame...\n",
      "make DataFrame complete ....\n",
      "label_encoding processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younghwan/.local/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:33:51] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "fitting  XGBoost Classifier  is complete...\n",
      "XGBoost Classifier score is : 0.9721577726218097\n",
      "Validation Result...\n",
      "hit:  838  miss :  24\n",
      "\n",
      "\n",
      "now test new data..\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "back_smash  -->  back_cut err_index :  73\n",
      "back_smash  -->  back_cut err_index :  74\n",
      "back_smash  -->  fo_smash err_index :  75\n",
      "back_smash  -->  back_cut err_index :  77\n",
      "back_smash  -->  fo_drive err_index :  84\n",
      "back_smash  -->  back_cut err_index :  85\n",
      "back_smash  -->  back_cut err_index :  86\n",
      "hit:  80  miss :  7\n"
     ]
    }
   ],
   "source": [
    "target_folder = 'swing'\n",
    "mode = 'XGBoost Classifier'\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "print('this model is ',mode,'\\n\\n')\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk(target_folder):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "print('load_file_complete')\n",
    "print('make DataFrame...')\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "print('make DataFrame complete ....')\n",
    "\n",
    "print('label_encoding processing...')\n",
    "encoder = LabelEncoder()\n",
    "arr_label = encoder.fit_transform(total_label)\n",
    "\n",
    "total_data = np.array(total_data)\n",
    "arr_label = np.array(arr_label)\n",
    "x_train,x_test,y_train,y_test = train_test_split(total_data,arr_label,test_size=0.2,random_state=0)\n",
    "\n",
    "nsamples, nx, ny = x_train.shape\n",
    "d2_train_dataset = x_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = x_test.shape\n",
    "d2_test_dataset = x_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "\n",
    "#####here to change####\n",
    "######################################################################\n",
    "\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 180,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "mod =xgb.XGBClassifier(\n",
    "                    n_estimators =space['n_estimators'], max_depth = space['max_depth'], gamma = space['gamma'],\n",
    "                    reg_alpha = space['reg_alpha'],min_child_weight=space['min_child_weight'],\n",
    "                    colsample_bytree=space['colsample_bytree'])\n",
    "    \n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## K-Nearest-Neighbor ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this model is  KNN \n",
      "\n",
      "\n",
      "load_file_complete\n",
      "make DataFrame...\n",
      "make DataFrame complete ....\n",
      "label_encoding processing...\n",
      "parameter Searching....\n",
      "fitting  KNN  is complete...\n",
      "KNN score is : 0.9860788863109049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       103\n",
      "           1       0.96      0.96      0.96        96\n",
      "           2       1.00      0.99      1.00       118\n",
      "           3       0.95      0.96      0.95        97\n",
      "           4       1.00      1.00      1.00        85\n",
      "           5       0.99      1.00      1.00       149\n",
      "           6       0.99      1.00      1.00       115\n",
      "           7       1.00      0.97      0.98        99\n",
      "\n",
      "    accuracy                           0.99       862\n",
      "   macro avg       0.99      0.98      0.99       862\n",
      "weighted avg       0.99      0.99      0.99       862\n",
      "\n",
      "Validation Result...\n",
      "hit:  850  miss :  12\n",
      "\n",
      "\n",
      "now test new data..\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "back_smash  -->  fo_smash err_index :  73\n",
      "back_smash  -->  back_cut err_index :  74\n",
      "back_smash  -->  fo_smash err_index :  75\n",
      "back_smash  -->  fo_smash err_index :  77\n",
      "back_smash  -->  back_short err_index :  78\n",
      "back_smash  -->  fo_smash err_index :  84\n",
      "back_smash  -->  fo_smash err_index :  85\n",
      "back_smash  -->  fo_smash err_index :  86\n",
      "hit:  79  miss :  8\n"
     ]
    }
   ],
   "source": [
    "target_folder = 'swing'\n",
    "mode = 'KNN'\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "print('this model is ',mode,'\\n\\n')\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk(target_folder):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "print('load_file_complete')\n",
    "print('make DataFrame...')\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "print('make DataFrame complete ....')\n",
    "\n",
    "print('label_encoding processing...')\n",
    "encoder = LabelEncoder()\n",
    "arr_label = encoder.fit_transform(total_label)\n",
    "\n",
    "total_data = np.array(total_data)\n",
    "arr_label = np.array(arr_label)\n",
    "x_train,x_test,y_train,y_test = train_test_split(total_data,arr_label,test_size=0.2,random_state=0)\n",
    "\n",
    "nsamples, nx, ny = x_train.shape\n",
    "d2_train_dataset = x_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = x_test.shape\n",
    "d2_test_dataset = x_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "print('parameter Searching....')\n",
    "#####here to change####\n",
    "######################################################################\n",
    "\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,30))\n",
    "p=[1,2]\n",
    "\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "mod = GridSearchCV(knn, hyperparameters, cv=10, n_jobs=-1)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "print(classification_report(prediction ,y_test))\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### LSTM_ sample Code ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "['back_cut' 'back_drive' 'back_short' 'back_smash' 'fo_cut' 'fo_drive'\n",
      " 'fo_short' 'fo_smash']\n",
      "Train on 4152 samples, validate on 1039 samples\n",
      "Epoch 1/100\n",
      "4152/4152 [==============================] - 4s 1ms/sample - loss: 0.6495 - acc: 0.7729 - val_loss: 0.2249 - val_acc: 0.9259\n",
      "Epoch 2/100\n",
      "4152/4152 [==============================] - 4s 884us/sample - loss: 0.1852 - acc: 0.9403 - val_loss: 0.1483 - val_acc: 0.9490\n",
      "Epoch 3/100\n",
      "4152/4152 [==============================] - 4s 877us/sample - loss: 0.1317 - acc: 0.9566 - val_loss: 0.1399 - val_acc: 0.9586\n",
      "Epoch 4/100\n",
      "4152/4152 [==============================] - 4s 887us/sample - loss: 0.1171 - acc: 0.9622 - val_loss: 0.1303 - val_acc: 0.9577\n",
      "Epoch 5/100\n",
      "4152/4152 [==============================] - 4s 884us/sample - loss: 0.1086 - acc: 0.9641 - val_loss: 0.1008 - val_acc: 0.9711\n",
      "Epoch 6/100\n",
      "4152/4152 [==============================] - 4s 889us/sample - loss: 0.0929 - acc: 0.9701 - val_loss: 0.0981 - val_acc: 0.9673\n",
      "Epoch 7/100\n",
      "4152/4152 [==============================] - 4s 887us/sample - loss: 0.0790 - acc: 0.9762 - val_loss: 0.0931 - val_acc: 0.9731\n",
      "Epoch 8/100\n",
      "4152/4152 [==============================] - 4s 885us/sample - loss: 0.0726 - acc: 0.9762 - val_loss: 0.0778 - val_acc: 0.9750\n",
      "Epoch 9/100\n",
      "4152/4152 [==============================] - 4s 889us/sample - loss: 0.0599 - acc: 0.9795 - val_loss: 0.0789 - val_acc: 0.9769\n",
      "Epoch 10/100\n",
      "4152/4152 [==============================] - 4s 891us/sample - loss: 0.0620 - acc: 0.9788 - val_loss: 0.0777 - val_acc: 0.9731\n",
      "Epoch 11/100\n",
      "4152/4152 [==============================] - 4s 885us/sample - loss: 0.0573 - acc: 0.9803 - val_loss: 0.0696 - val_acc: 0.9759\n",
      "Epoch 12/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0489 - acc: 0.9817 - val_loss: 0.0840 - val_acc: 0.9731\n",
      "Epoch 13/100\n",
      "4152/4152 [==============================] - 4s 889us/sample - loss: 0.0458 - acc: 0.9851 - val_loss: 0.0707 - val_acc: 0.9779\n",
      "Epoch 14/100\n",
      "4152/4152 [==============================] - 4s 891us/sample - loss: 0.0593 - acc: 0.9788 - val_loss: 0.0673 - val_acc: 0.9711\n",
      "Epoch 15/100\n",
      "4152/4152 [==============================] - 4s 891us/sample - loss: 0.0418 - acc: 0.9846 - val_loss: 0.0821 - val_acc: 0.9663\n",
      "Epoch 16/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0367 - acc: 0.9872 - val_loss: 0.0613 - val_acc: 0.9798\n",
      "Epoch 17/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0300 - acc: 0.9899 - val_loss: 0.0626 - val_acc: 0.9788\n",
      "Epoch 18/100\n",
      "4152/4152 [==============================] - 4s 886us/sample - loss: 0.0291 - acc: 0.9906 - val_loss: 0.0701 - val_acc: 0.9788\n",
      "Epoch 19/100\n",
      "4152/4152 [==============================] - 4s 882us/sample - loss: 0.0316 - acc: 0.9884 - val_loss: 0.0608 - val_acc: 0.9798\n",
      "Epoch 20/100\n",
      "4152/4152 [==============================] - 4s 891us/sample - loss: 0.0299 - acc: 0.9908 - val_loss: 0.0674 - val_acc: 0.9779\n",
      "Epoch 21/100\n",
      "4152/4152 [==============================] - 4s 901us/sample - loss: 0.0406 - acc: 0.9860 - val_loss: 0.0636 - val_acc: 0.9808\n",
      "Epoch 22/100\n",
      "4152/4152 [==============================] - 4s 889us/sample - loss: 0.0325 - acc: 0.9889 - val_loss: 0.1003 - val_acc: 0.9663\n",
      "Epoch 23/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0300 - acc: 0.9880 - val_loss: 0.0648 - val_acc: 0.9779\n",
      "Epoch 24/100\n",
      "4152/4152 [==============================] - 4s 887us/sample - loss: 0.0307 - acc: 0.9889 - val_loss: 0.0679 - val_acc: 0.9779\n",
      "Epoch 25/100\n",
      "4152/4152 [==============================] - 4s 889us/sample - loss: 0.0244 - acc: 0.9906 - val_loss: 0.1125 - val_acc: 0.9625\n",
      "Epoch 26/100\n",
      "4152/4152 [==============================] - 4s 896us/sample - loss: 0.0308 - acc: 0.9872 - val_loss: 0.0653 - val_acc: 0.9827\n",
      "Epoch 27/100\n",
      "4152/4152 [==============================] - 4s 880us/sample - loss: 0.0227 - acc: 0.9908 - val_loss: 0.0624 - val_acc: 0.9769\n",
      "Epoch 28/100\n",
      "4152/4152 [==============================] - 4s 896us/sample - loss: 0.0233 - acc: 0.9921 - val_loss: 0.0785 - val_acc: 0.9788\n",
      "Epoch 29/100\n",
      "4152/4152 [==============================] - 4s 886us/sample - loss: 0.0184 - acc: 0.9942 - val_loss: 0.0696 - val_acc: 0.9817\n",
      "Epoch 30/100\n",
      "4152/4152 [==============================] - 4s 889us/sample - loss: 0.0304 - acc: 0.9887 - val_loss: 0.0936 - val_acc: 0.9731\n",
      "Epoch 31/100\n",
      "4152/4152 [==============================] - 4s 890us/sample - loss: 0.0218 - acc: 0.9913 - val_loss: 0.0649 - val_acc: 0.9817\n",
      "Epoch 32/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0209 - acc: 0.9916 - val_loss: 0.0872 - val_acc: 0.9788\n",
      "Epoch 33/100\n",
      "4152/4152 [==============================] - 4s 885us/sample - loss: 0.0169 - acc: 0.9947 - val_loss: 0.0711 - val_acc: 0.9788\n",
      "Epoch 34/100\n",
      "4152/4152 [==============================] - 4s 893us/sample - loss: 0.0217 - acc: 0.9930 - val_loss: 0.0642 - val_acc: 0.9788\n",
      "Epoch 35/100\n",
      "4152/4152 [==============================] - 4s 896us/sample - loss: 0.0132 - acc: 0.9952 - val_loss: 0.0682 - val_acc: 0.9788\n",
      "Epoch 36/100\n",
      "4152/4152 [==============================] - 4s 892us/sample - loss: 0.0172 - acc: 0.9940 - val_loss: 0.0647 - val_acc: 0.9817\n",
      "Epoch 37/100\n",
      "4152/4152 [==============================] - 4s 895us/sample - loss: 0.0108 - acc: 0.9954 - val_loss: 0.0686 - val_acc: 0.9808\n",
      "Epoch 38/100\n",
      "4152/4152 [==============================] - 4s 882us/sample - loss: 0.0090 - acc: 0.9974 - val_loss: 0.0678 - val_acc: 0.9817\n",
      "Epoch 39/100\n",
      "4152/4152 [==============================] - 4s 887us/sample - loss: 0.0181 - acc: 0.9933 - val_loss: 0.0507 - val_acc: 0.9856\n",
      "Epoch 40/100\n",
      "4152/4152 [==============================] - 4s 890us/sample - loss: 0.0085 - acc: 0.9974 - val_loss: 0.0650 - val_acc: 0.9808\n",
      "Epoch 41/100\n",
      "4152/4152 [==============================] - 4s 884us/sample - loss: 0.0213 - acc: 0.9930 - val_loss: 0.0568 - val_acc: 0.9856\n",
      "Epoch 42/100\n",
      "4152/4152 [==============================] - 4s 886us/sample - loss: 0.0181 - acc: 0.9935 - val_loss: 0.0637 - val_acc: 0.9798\n",
      "Epoch 43/100\n",
      "4152/4152 [==============================] - 4s 891us/sample - loss: 0.0089 - acc: 0.9969 - val_loss: 0.0528 - val_acc: 0.9846\n",
      "Epoch 44/100\n",
      "4152/4152 [==============================] - 4s 885us/sample - loss: 0.0105 - acc: 0.9969 - val_loss: 0.0714 - val_acc: 0.9827\n",
      "Epoch 45/100\n",
      "4152/4152 [==============================] - 4s 881us/sample - loss: 0.0105 - acc: 0.9961 - val_loss: 0.0562 - val_acc: 0.9856\n",
      "Epoch 46/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0134 - acc: 0.9949 - val_loss: 0.0807 - val_acc: 0.9788\n",
      "Epoch 47/100\n",
      "4152/4152 [==============================] - 4s 890us/sample - loss: 0.0123 - acc: 0.9942 - val_loss: 0.0859 - val_acc: 0.9788\n",
      "Epoch 48/100\n",
      "4152/4152 [==============================] - 4s 890us/sample - loss: 0.0066 - acc: 0.9978 - val_loss: 0.0561 - val_acc: 0.9865\n",
      "Epoch 49/100\n",
      "4152/4152 [==============================] - 4s 875us/sample - loss: 0.0146 - acc: 0.9952 - val_loss: 0.0727 - val_acc: 0.9788\n",
      "Epoch 50/100\n",
      "4152/4152 [==============================] - 4s 889us/sample - loss: 0.0096 - acc: 0.9961 - val_loss: 0.0653 - val_acc: 0.9769\n",
      "Epoch 51/100\n",
      "4152/4152 [==============================] - 4s 893us/sample - loss: 0.0144 - acc: 0.9949 - val_loss: 0.0793 - val_acc: 0.9808\n",
      "Epoch 52/100\n",
      "4152/4152 [==============================] - 4s 886us/sample - loss: 0.0086 - acc: 0.9971 - val_loss: 0.0636 - val_acc: 0.9836\n",
      "Epoch 53/100\n",
      "4152/4152 [==============================] - 4s 882us/sample - loss: 0.0078 - acc: 0.9976 - val_loss: 0.0564 - val_acc: 0.9856\n",
      "Epoch 54/100\n",
      "4152/4152 [==============================] - 4s 885us/sample - loss: 0.0055 - acc: 0.9981 - val_loss: 0.0674 - val_acc: 0.9808\n",
      "Epoch 55/100\n",
      "4152/4152 [==============================] - 4s 893us/sample - loss: 0.0198 - acc: 0.9940 - val_loss: 0.1100 - val_acc: 0.9769\n",
      "Epoch 56/100\n",
      "4152/4152 [==============================] - 4s 883us/sample - loss: 0.0087 - acc: 0.9974 - val_loss: 0.0724 - val_acc: 0.9836\n",
      "Epoch 57/100\n",
      "4152/4152 [==============================] - 4s 879us/sample - loss: 0.0123 - acc: 0.9966 - val_loss: 0.0568 - val_acc: 0.9836\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4152/4152 [==============================] - 4s 885us/sample - loss: 0.0079 - acc: 0.9966 - val_loss: 0.0878 - val_acc: 0.9769\n",
      "Epoch 59/100\n",
      "4152/4152 [==============================] - 4s 886us/sample - loss: 0.0101 - acc: 0.9978 - val_loss: 0.0578 - val_acc: 0.9836\n",
      "Epoch 60/100\n",
      "4152/4152 [==============================] - 4s 894us/sample - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0664 - val_acc: 0.9817\n",
      "Epoch 61/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0710 - val_acc: 0.9846\n",
      "Epoch 62/100\n",
      "4152/4152 [==============================] - 4s 892us/sample - loss: 0.0089 - acc: 0.9971 - val_loss: 0.0959 - val_acc: 0.9779\n",
      "Epoch 63/100\n",
      "4152/4152 [==============================] - 4s 886us/sample - loss: 0.0241 - acc: 0.9916 - val_loss: 0.1065 - val_acc: 0.9711\n",
      "Epoch 64/100\n",
      "4152/4152 [==============================] - 4s 883us/sample - loss: 0.0148 - acc: 0.9952 - val_loss: 0.0721 - val_acc: 0.9846\n",
      "Epoch 65/100\n",
      "4152/4152 [==============================] - 4s 887us/sample - loss: 0.0086 - acc: 0.9971 - val_loss: 0.0591 - val_acc: 0.9836\n",
      "Epoch 66/100\n",
      "4152/4152 [==============================] - 4s 901us/sample - loss: 0.0094 - acc: 0.9974 - val_loss: 0.0639 - val_acc: 0.9817\n",
      "Epoch 67/100\n",
      "4152/4152 [==============================] - 4s 891us/sample - loss: 0.0041 - acc: 0.9986 - val_loss: 0.0598 - val_acc: 0.9836\n",
      "Epoch 68/100\n",
      "4152/4152 [==============================] - 4s 884us/sample - loss: 0.0065 - acc: 0.9976 - val_loss: 0.0617 - val_acc: 0.9836\n",
      "Epoch 69/100\n",
      "4152/4152 [==============================] - 4s 891us/sample - loss: 0.0032 - acc: 0.9988 - val_loss: 0.0622 - val_acc: 0.9836\n",
      "Epoch 70/100\n",
      "4152/4152 [==============================] - 4s 885us/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 0.0580 - val_acc: 0.9865\n",
      "Epoch 71/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0071 - acc: 0.9969 - val_loss: 0.0559 - val_acc: 0.9836\n",
      "Epoch 72/100\n",
      "4152/4152 [==============================] - 4s 892us/sample - loss: 0.0119 - acc: 0.9954 - val_loss: 0.0894 - val_acc: 0.9721\n",
      "Epoch 73/100\n",
      "4152/4152 [==============================] - 4s 883us/sample - loss: 0.0236 - acc: 0.9928 - val_loss: 0.0608 - val_acc: 0.9836\n",
      "Epoch 74/100\n",
      "4152/4152 [==============================] - 4s 890us/sample - loss: 0.0100 - acc: 0.9971 - val_loss: 0.0645 - val_acc: 0.9817\n",
      "Epoch 75/100\n",
      "4152/4152 [==============================] - 4s 893us/sample - loss: 0.0160 - acc: 0.9952 - val_loss: 0.0680 - val_acc: 0.9817\n",
      "Epoch 76/100\n",
      "4152/4152 [==============================] - 4s 890us/sample - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0670 - val_acc: 0.9808\n",
      "Epoch 77/100\n",
      "4152/4152 [==============================] - 4s 894us/sample - loss: 0.0056 - acc: 0.9988 - val_loss: 0.0535 - val_acc: 0.9875\n",
      "Epoch 78/100\n",
      "4152/4152 [==============================] - 4s 885us/sample - loss: 0.0067 - acc: 0.9974 - val_loss: 0.0607 - val_acc: 0.9865\n",
      "Epoch 79/100\n",
      "4152/4152 [==============================] - 4s 889us/sample - loss: 0.0067 - acc: 0.9969 - val_loss: 0.0561 - val_acc: 0.9865\n",
      "Epoch 80/100\n",
      "4152/4152 [==============================] - 4s 883us/sample - loss: 0.0025 - acc: 0.9990 - val_loss: 0.0534 - val_acc: 0.9865\n",
      "Epoch 81/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0024 - acc: 0.9990 - val_loss: 0.0629 - val_acc: 0.9865\n",
      "Epoch 82/100\n",
      "4152/4152 [==============================] - 4s 882us/sample - loss: 0.0035 - acc: 0.9995 - val_loss: 0.0556 - val_acc: 0.9865\n",
      "Epoch 83/100\n",
      "4152/4152 [==============================] - 4s 878us/sample - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0551 - val_acc: 0.9865\n",
      "Epoch 84/100\n",
      "4152/4152 [==============================] - 4s 890us/sample - loss: 6.2442e-04 - acc: 1.0000 - val_loss: 0.0503 - val_acc: 0.9875\n",
      "Epoch 85/100\n",
      "4152/4152 [==============================] - 4s 873us/sample - loss: 5.8767e-04 - acc: 1.0000 - val_loss: 0.0545 - val_acc: 0.9865\n",
      "Epoch 86/100\n",
      "4152/4152 [==============================] - 4s 889us/sample - loss: 6.0742e-04 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 0.9875\n",
      "Epoch 87/100\n",
      "4152/4152 [==============================] - 4s 883us/sample - loss: 0.0025 - acc: 0.9995 - val_loss: 0.0809 - val_acc: 0.9817\n",
      "Epoch 88/100\n",
      "4152/4152 [==============================] - 4s 888us/sample - loss: 0.0252 - acc: 0.9923 - val_loss: 0.0773 - val_acc: 0.9817\n",
      "Epoch 89/100\n",
      "4152/4152 [==============================] - 4s 891us/sample - loss: 0.0097 - acc: 0.9966 - val_loss: 0.0791 - val_acc: 0.9817\n",
      "Epoch 90/100\n",
      "4152/4152 [==============================] - 4s 880us/sample - loss: 0.0090 - acc: 0.9969 - val_loss: 0.0557 - val_acc: 0.9875\n",
      "Epoch 91/100\n",
      "4152/4152 [==============================] - 4s 896us/sample - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0659 - val_acc: 0.9846\n",
      "Epoch 92/100\n",
      "4152/4152 [==============================] - 4s 890us/sample - loss: 0.0033 - acc: 0.9988 - val_loss: 0.0570 - val_acc: 0.9865\n",
      "Epoch 93/100\n",
      "4152/4152 [==============================] - 4s 879us/sample - loss: 0.0019 - acc: 0.9993 - val_loss: 0.0619 - val_acc: 0.9856\n",
      "Epoch 94/100\n",
      "4152/4152 [==============================] - 4s 884us/sample - loss: 6.4313e-04 - acc: 1.0000 - val_loss: 0.0651 - val_acc: 0.9846\n",
      "Epoch 95/100\n",
      "4152/4152 [==============================] - 4s 883us/sample - loss: 4.4040e-04 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9856\n",
      "Epoch 96/100\n",
      "4152/4152 [==============================] - 4s 880us/sample - loss: 5.2292e-04 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 0.9865\n",
      "Epoch 97/100\n",
      "4152/4152 [==============================] - 4s 884us/sample - loss: 3.5329e-04 - acc: 1.0000 - val_loss: 0.0713 - val_acc: 0.9875\n",
      "Epoch 98/100\n",
      "4152/4152 [==============================] - 4s 884us/sample - loss: 2.7917e-04 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 0.9865\n",
      "Epoch 99/100\n",
      "4152/4152 [==============================] - 4s 887us/sample - loss: 2.1142e-04 - acc: 1.0000 - val_loss: 0.0684 - val_acc: 0.9865\n",
      "Epoch 100/100\n",
      "4152/4152 [==============================] - 4s 890us/sample - loss: 0.0034 - acc: 0.9993 - val_loss: 0.0997 - val_acc: 0.9836\n",
      "Best:  [0.77288055, 0.94026977, 0.9566474, 0.9621869, 0.96411365, 0.97013485, 0.97615606, 0.97615606, 0.97952795, 0.9788054, 0.9802505, 0.9816956, 0.9850674, 0.9788054, 0.98458576, 0.98723507, 0.9898844, 0.99060696, 0.9884393, 0.99084777, 0.9860308, 0.988921, 0.9879576, 0.988921, 0.99060696, 0.98723507, 0.99084777, 0.992052, 0.99421966, 0.9886802, 0.9913295, 0.99157035, 0.9947013, 0.9930154, 0.99518305, 0.9939788, 0.9954239, 0.9973507, 0.9932563, 0.9973507, 0.9930154, 0.99349713, 0.99686897, 0.99686897, 0.99614644, 0.9949422, 0.99421966, 0.99783236, 0.99518305, 0.99614644, 0.9949422, 0.99710983, 0.9975915, 0.9980732, 0.9939788, 0.9973507, 0.9966281, 0.9966281, 0.99783236, 0.9990366, 0.9995183, 0.99710983, 0.99157035, 0.99518305, 0.99710983, 0.9973507, 0.9985549, 0.9975915, 0.99879575, 0.9995183, 0.99686897, 0.9954239, 0.99277455, 0.99710983, 0.99518305, 0.99686897, 0.99879575, 0.9973507, 0.99686897, 0.9990366, 0.9990366, 0.9995183, 1.0, 1.0, 1.0, 1.0, 0.9995183, 0.9922929, 0.9966281, 0.99686897, 0.99879575, 0.99879575, 0.9992775, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9992775]\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "back_smash  -->  fo_drive         err_index number :  73\n",
      "back_smash  -->  fo_drive         err_index number :  74\n",
      "back_smash  -->  fo_drive         err_index number :  75\n",
      "back_smash  -->  fo_drive         err_index number :  77\n",
      "back_smash  -->  fo_smash         err_index number :  84\n",
      "back_smash  -->  fo_drive         err_index number :  85\n",
      "back_smash  -->  fo_drive         err_index number :  86\n",
      "hit:  80  miss :  7 percent :  91.95402298850574\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8 \n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('swing'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "arr_label = encoder.fit_transform(total_label)\n",
    "print(encoder.classes_)\n",
    "\n",
    "total_data = np.array(total_data)\n",
    "arr_label = np.array(arr_label)\n",
    "x_train,x_test,y_train,y_test = train_test_split(total_data,arr_label,test_size=0.2,random_state=0)\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential() # Sequeatial Model \n",
    "    model.add(LSTM(60, input_shape=(60,3),return_sequences = True)) # (timestep, feature) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv1D(256,\n",
    "                     2,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "    # 3. 모델 학습과정 설정하기\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(x_train, y_train, epochs=100, batch_size=64, validation_data=(x_test, y_test))\n",
    "    model.save('model.h5')\n",
    "    model.save_weights('model_weights.h5')\n",
    "print(\"Best: \" ,(hist.history['acc']))\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "#sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = model.predict(total_data)\n",
    "sample_pred = np.argmax(sample_pred,axis=-1)\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'        err_index number : ',x)\n",
    "\n",
    "        \n",
    "print('hit: ',hit,' miss : ',miss,'percent : ',(100*hit)/(hit+miss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
