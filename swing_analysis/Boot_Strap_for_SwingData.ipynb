{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.naive_bayes as nb\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import LSTM,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout,Input,Dense,Activation,Flatten,SeparableConv2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/12331 [00:00<02:53, 71.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this model is  LSTM model \n",
      "\n",
      "\n",
      "load_file_complete\n",
      "make DataFrame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12331/12331 [02:25<00:00, 84.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make DataFrame complete ....\n",
      "label_encoding processing...\n",
      "Done....\n",
      "12331\n"
     ]
    }
   ],
   "source": [
    "target_folder = 'swing'\n",
    "mode = 'LSTM model'\n",
    "\n",
    "print('this model is ',mode,'\\n\\n')\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk(target_folder):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "print('load_file_complete')\n",
    "print('make DataFrame...')\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in tqdm(filename_in_dir):\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp.append(df['GX'][i]/1000)\n",
    "        tmp.append(df['GY'][i]/1000)\n",
    "        tmp.append(df['GZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "print('make DataFrame complete ....')\n",
    "\n",
    "print('label_encoding processing...')\n",
    "encoder = LabelEncoder()\n",
    "arr_label = encoder.fit_transform(total_label)\n",
    "\n",
    "total_data = np.array(total_data)\n",
    "arr_label = np.array(arr_label)\n",
    "print('Done....')\n",
    "print(len(arr_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1838, 3407, 5041, 6741, 8170, 9491, 10885, 12330]\n"
     ]
    }
   ],
   "source": [
    "tmp_li = []\n",
    "tmp_li.append(0)\n",
    "for x in range(len(arr_label)-1):\n",
    "    if arr_label[x]!=arr_label[x+1]:\n",
    "        tmp_li.append(x)\n",
    "tmp_li.append(len(arr_label)-1)\n",
    "print(tmp_li)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_data = []\n",
    "div_label = []\n",
    "for x in range(len(tmp_li)-1):\n",
    "    div_tmp =[]\n",
    "    div_la = []\n",
    "    for y in range(tmp_li[x]+1,tmp_li[x+1]+1):\n",
    "        div_tmp.append(total_data[y])\n",
    "        div_la.append(arr_label[y])\n",
    "    div_data.append(div_tmp)\n",
    "    div_label.append(div_la)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "boot = []\n",
    "label = []\n",
    "t_li = [5,1,7,6,0,4,2,3]\n",
    "for x in range(len(div_data)):\n",
    "    tmp = resample(div_data[x],replace=True,n_samples = 5000,random_state=1)\n",
    "    boot+=tmp\n",
    "    label += list(t_li[x] for i in range(5000))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Train on 41864 samples, validate on 10467 samples\n",
      "Epoch 1/100\n",
      "41864/41864 [==============================] - 38s 907us/sample - loss: 0.1632 - acc: 0.9458 - val_loss: 0.0770 - val_acc: 0.9795\n",
      "Epoch 2/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0630 - acc: 0.9812 - val_loss: 0.0449 - val_acc: 0.9882\n",
      "Epoch 3/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0418 - acc: 0.9876 - val_loss: 0.0423 - val_acc: 0.9866\n",
      "Epoch 4/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0338 - acc: 0.9899 - val_loss: 0.0280 - val_acc: 0.9924\n",
      "Epoch 5/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0257 - acc: 0.9920 - val_loss: 0.0292 - val_acc: 0.9919\n",
      "Epoch 6/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0210 - acc: 0.9933 - val_loss: 0.0305 - val_acc: 0.9893\n",
      "Epoch 7/100\n",
      "41864/41864 [==============================] - 37s 886us/sample - loss: 0.0223 - acc: 0.9929 - val_loss: 0.0211 - val_acc: 0.9934\n",
      "Epoch 8/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0157 - acc: 0.9951 - val_loss: 0.0437 - val_acc: 0.9851\n",
      "Epoch 9/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0160 - acc: 0.9952 - val_loss: 0.0230 - val_acc: 0.9923\n",
      "Epoch 10/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0142 - acc: 0.9957 - val_loss: 0.0371 - val_acc: 0.9884\n",
      "Epoch 11/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0127 - acc: 0.9957 - val_loss: 0.0118 - val_acc: 0.9951\n",
      "Epoch 12/100\n",
      "41864/41864 [==============================] - 37s 885us/sample - loss: 0.0104 - acc: 0.9970 - val_loss: 0.0092 - val_acc: 0.9969\n",
      "Epoch 13/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0077 - acc: 0.9974 - val_loss: 0.0207 - val_acc: 0.9929\n",
      "Epoch 14/100\n",
      "41864/41864 [==============================] - 37s 892us/sample - loss: 0.0129 - acc: 0.9962 - val_loss: 0.0179 - val_acc: 0.9948\n",
      "Epoch 15/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0091 - acc: 0.9972 - val_loss: 0.0199 - val_acc: 0.9936\n",
      "Epoch 16/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0060 - acc: 0.9981 - val_loss: 0.0077 - val_acc: 0.9972\n",
      "Epoch 17/100\n",
      "41864/41864 [==============================] - 37s 887us/sample - loss: 0.0101 - acc: 0.9972 - val_loss: 0.0322 - val_acc: 0.9909\n",
      "Epoch 18/100\n",
      "41864/41864 [==============================] - 37s 885us/sample - loss: 0.0123 - acc: 0.9962 - val_loss: 0.0281 - val_acc: 0.9908\n",
      "Epoch 19/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0073 - acc: 0.9978 - val_loss: 0.0082 - val_acc: 0.9973\n",
      "Epoch 20/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0065 - acc: 0.9982 - val_loss: 0.0168 - val_acc: 0.9954\n",
      "Epoch 21/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0091 - acc: 0.9973 - val_loss: 0.0317 - val_acc: 0.9913\n",
      "Epoch 22/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0039 - acc: 0.9987 - val_loss: 0.0099 - val_acc: 0.9972\n",
      "Epoch 23/100\n",
      "41864/41864 [==============================] - 37s 893us/sample - loss: 0.0072 - acc: 0.9979 - val_loss: 0.0158 - val_acc: 0.9964\n",
      "Epoch 24/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0116 - acc: 0.9966 - val_loss: 0.0100 - val_acc: 0.9975\n",
      "Epoch 25/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0062 - val_acc: 0.9982\n",
      "Epoch 26/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0070 - val_acc: 0.9982\n",
      "Epoch 27/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0169 - val_acc: 0.9948\n",
      "Epoch 28/100\n",
      "41864/41864 [==============================] - 37s 886us/sample - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0103 - val_acc: 0.9975\n",
      "Epoch 29/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0155 - val_acc: 0.9948\n",
      "Epoch 30/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0667 - val_acc: 0.9920\n",
      "Epoch 31/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0093 - acc: 0.9973 - val_loss: 0.0170 - val_acc: 0.9943\n",
      "Epoch 32/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0080 - acc: 0.9975 - val_loss: 0.0092 - val_acc: 0.9968\n",
      "Epoch 33/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0024 - acc: 0.9994 - val_loss: 0.0113 - val_acc: 0.9963\n",
      "Epoch 34/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0017 - acc: 0.9996 - val_loss: 0.0142 - val_acc: 0.9961\n",
      "Epoch 35/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0057 - acc: 0.9985 - val_loss: 0.0083 - val_acc: 0.9972\n",
      "Epoch 36/100\n",
      "41864/41864 [==============================] - 37s 892us/sample - loss: 0.0028 - acc: 0.9991 - val_loss: 0.0210 - val_acc: 0.9942\n",
      "Epoch 37/100\n",
      "41864/41864 [==============================] - 37s 887us/sample - loss: 0.0081 - acc: 0.9979 - val_loss: 0.0165 - val_acc: 0.9956\n",
      "Epoch 38/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0131 - val_acc: 0.9966\n",
      "Epoch 39/100\n",
      "41864/41864 [==============================] - 37s 887us/sample - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0084 - val_acc: 0.9977\n",
      "Epoch 40/100\n",
      "41864/41864 [==============================] - 37s 885us/sample - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0119 - val_acc: 0.9975\n",
      "Epoch 41/100\n",
      "41864/41864 [==============================] - 37s 885us/sample - loss: 0.0034 - acc: 0.9990 - val_loss: 0.0110 - val_acc: 0.9978\n",
      "Epoch 42/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0044 - acc: 0.9988 - val_loss: 0.0261 - val_acc: 0.9925\n",
      "Epoch 43/100\n",
      "41864/41864 [==============================] - 37s 886us/sample - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0169 - val_acc: 0.9955\n",
      "Epoch 44/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 7.6040e-04 - acc: 0.9998 - val_loss: 0.0145 - val_acc: 0.9967\n",
      "Epoch 45/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0088 - acc: 0.9976 - val_loss: 0.0272 - val_acc: 0.9925\n",
      "Epoch 46/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0058 - acc: 0.9984 - val_loss: 0.0203 - val_acc: 0.9933\n",
      "Epoch 47/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0049 - acc: 0.9983 - val_loss: 0.0149 - val_acc: 0.9954\n",
      "Epoch 48/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0020 - acc: 0.9995 - val_loss: 0.0117 - val_acc: 0.9976\n",
      "Epoch 49/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 9.2126e-04 - acc: 0.9999 - val_loss: 0.0101 - val_acc: 0.9971\n",
      "Epoch 50/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 3.4336e-04 - acc: 0.9999 - val_loss: 0.0121 - val_acc: 0.9968\n",
      "Epoch 51/100\n",
      "41864/41864 [==============================] - 37s 886us/sample - loss: 8.3105e-05 - acc: 1.0000 - val_loss: 0.0105 - val_acc: 0.9975\n",
      "Epoch 52/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0099 - acc: 0.9974 - val_loss: 0.0208 - val_acc: 0.9946\n",
      "Epoch 53/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0051 - acc: 0.9987 - val_loss: 0.0242 - val_acc: 0.9920\n",
      "Epoch 54/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0012 - acc: 0.9997 - val_loss: 0.0114 - val_acc: 0.9971\n",
      "Epoch 55/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 8.7133e-04 - acc: 0.9998 - val_loss: 0.0126 - val_acc: 0.9965\n",
      "Epoch 56/100\n",
      "41864/41864 [==============================] - 37s 886us/sample - loss: 3.2892e-04 - acc: 0.9999 - val_loss: 0.0106 - val_acc: 0.9977\n",
      "Epoch 57/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0085 - acc: 0.9976 - val_loss: 0.0131 - val_acc: 0.9964\n",
      "Epoch 58/100\n",
      "41864/41864 [==============================] - 37s 887us/sample - loss: 0.0011 - acc: 0.9997 - val_loss: 0.0092 - val_acc: 0.9973\n",
      "Epoch 59/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 3.5215e-04 - acc: 1.0000 - val_loss: 0.0101 - val_acc: 0.9975\n",
      "Epoch 60/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 1.6762e-04 - acc: 1.0000 - val_loss: 0.0085 - val_acc: 0.9981\n",
      "Epoch 61/100\n",
      "41864/41864 [==============================] - 37s 893us/sample - loss: 0.0022 - acc: 0.9993 - val_loss: 0.0218 - val_acc: 0.9955\n",
      "Epoch 62/100\n",
      "41864/41864 [==============================] - 37s 887us/sample - loss: 0.0075 - acc: 0.9982 - val_loss: 0.0127 - val_acc: 0.9962\n",
      "Epoch 63/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0022 - acc: 0.9995 - val_loss: 0.0099 - val_acc: 0.9975\n",
      "Epoch 64/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0269 - val_acc: 0.9939\n",
      "Epoch 65/100\n",
      "41864/41864 [==============================] - 37s 892us/sample - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0085 - val_acc: 0.9979\n",
      "Epoch 66/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 8.5223e-04 - acc: 0.9999 - val_loss: 0.0145 - val_acc: 0.9959\n",
      "Epoch 67/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0025 - acc: 0.9992 - val_loss: 0.0226 - val_acc: 0.9934\n",
      "Epoch 68/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0040 - acc: 0.9991 - val_loss: 0.0268 - val_acc: 0.9902\n",
      "Epoch 69/100\n",
      "41864/41864 [==============================] - 37s 887us/sample - loss: 0.0048 - acc: 0.9991 - val_loss: 0.0241 - val_acc: 0.9928\n",
      "Epoch 70/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0016 - acc: 0.9994 - val_loss: 0.0154 - val_acc: 0.9957\n",
      "Epoch 71/100\n",
      "41864/41864 [==============================] - 37s 887us/sample - loss: 2.9594e-04 - acc: 1.0000 - val_loss: 0.0140 - val_acc: 0.9972\n",
      "Epoch 72/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0020 - acc: 0.9996 - val_loss: 0.0140 - val_acc: 0.9959\n",
      "Epoch 73/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0170 - val_acc: 0.9958\n",
      "Epoch 74/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0046 - acc: 0.9985 - val_loss: 0.0368 - val_acc: 0.9920\n",
      "Epoch 75/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0041 - val_acc: 0.9986\n",
      "Epoch 76/100\n",
      "41864/41864 [==============================] - 37s 884us/sample - loss: 0.0017 - acc: 0.9995 - val_loss: 0.0097 - val_acc: 0.9972\n",
      "Epoch 77/100\n",
      "41864/41864 [==============================] - 37s 892us/sample - loss: 0.0039 - acc: 0.9989 - val_loss: 0.0120 - val_acc: 0.9963\n",
      "Epoch 78/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0181 - val_acc: 0.9952\n",
      "Epoch 79/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 0.0010 - acc: 0.9997 - val_loss: 0.0049 - val_acc: 0.9984\n",
      "Epoch 80/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 3.2816e-04 - acc: 0.9999 - val_loss: 0.0037 - val_acc: 0.9982\n",
      "Epoch 81/100\n",
      "41864/41864 [==============================] - 37s 887us/sample - loss: 8.6196e-04 - acc: 0.9997 - val_loss: 0.0083 - val_acc: 0.9973\n",
      "Epoch 82/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 0.0060 - acc: 0.9984 - val_loss: 0.0297 - val_acc: 0.9917\n",
      "Epoch 83/100\n",
      "41864/41864 [==============================] - 37s 886us/sample - loss: 0.0030 - acc: 0.9993 - val_loss: 0.0224 - val_acc: 0.9939\n",
      "Epoch 84/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0016 - acc: 0.9995 - val_loss: 0.0260 - val_acc: 0.9928\n",
      "Epoch 85/100\n",
      "41864/41864 [==============================] - 37s 895us/sample - loss: 7.8979e-04 - acc: 0.9998 - val_loss: 0.0037 - val_acc: 0.9988\n",
      "Epoch 86/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 4.0263e-04 - acc: 0.9999 - val_loss: 0.0066 - val_acc: 0.9982\n",
      "Epoch 87/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 0.0257 - val_acc: 0.9946\n",
      "Epoch 88/100\n",
      "41864/41864 [==============================] - 37s 885us/sample - loss: 0.0028 - acc: 0.9992 - val_loss: 0.0133 - val_acc: 0.9972\n",
      "Epoch 89/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 6.8925e-04 - acc: 0.9998 - val_loss: 0.0226 - val_acc: 0.9946\n",
      "Epoch 90/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 8.2072e-04 - acc: 0.9998 - val_loss: 0.0127 - val_acc: 0.9968\n",
      "Epoch 91/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 9.6187e-04 - acc: 0.9997 - val_loss: 0.0289 - val_acc: 0.9940\n",
      "Epoch 92/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0029 - acc: 0.9995 - val_loss: 0.0212 - val_acc: 0.9937\n",
      "Epoch 93/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 6.2967e-04 - acc: 0.9999 - val_loss: 0.0232 - val_acc: 0.9938\n",
      "Epoch 94/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 5.2436e-04 - acc: 0.9999 - val_loss: 0.0677 - val_acc: 0.9861\n",
      "Epoch 95/100\n",
      "41864/41864 [==============================] - 37s 885us/sample - loss: 0.0058 - acc: 0.9982 - val_loss: 0.0192 - val_acc: 0.9952\n",
      "Epoch 96/100\n",
      "41864/41864 [==============================] - 37s 887us/sample - loss: 0.0049 - acc: 0.9987 - val_loss: 0.0156 - val_acc: 0.9956\n",
      "Epoch 97/100\n",
      "41864/41864 [==============================] - 37s 891us/sample - loss: 7.9852e-04 - acc: 0.9998 - val_loss: 0.0056 - val_acc: 0.9984\n",
      "Epoch 98/100\n",
      "41864/41864 [==============================] - 37s 889us/sample - loss: 1.2010e-04 - acc: 1.0000 - val_loss: 0.0060 - val_acc: 0.9985\n",
      "Epoch 99/100\n",
      "41864/41864 [==============================] - 37s 888us/sample - loss: 1.3404e-04 - acc: 1.0000 - val_loss: 0.0101 - val_acc: 0.9965\n",
      "Epoch 100/100\n",
      "41864/41864 [==============================] - 37s 890us/sample - loss: 0.0041 - acc: 0.9990 - val_loss: 0.0291 - val_acc: 0.9933\n",
      "Best:  [0.94582456, 0.9812249, 0.9876027, 0.9898959, 0.9920218, 0.9932878, 0.9929056, 0.9951032, 0.9951987, 0.99574816, 0.99574816, 0.99701416, 0.9974441, 0.9961542, 0.99718136, 0.9980891, 0.99720526, 0.9961542, 0.99780244, 0.99823236, 0.99732465, 0.998734, 0.9979457, 0.996632, 0.99925953, 0.99957, 0.99758744, 0.9989251, 0.9987101, 0.9995939, 0.99734855, 0.99753964, 0.9994028, 0.99957, 0.99847126, 0.99906844, 0.9978502, 0.99906844, 0.9990923, 0.999164, 0.99904454, 0.9987579, 0.99957, 0.999785, 0.9976352, 0.99842346, 0.998304, 0.99947447, 0.99988055, 0.99992836, 1.0, 0.99739635, 0.9987101, 0.99971336, 0.9998328, 0.99990445, 0.9976352, 0.99971336, 0.9999761, 0.9999761, 0.9993312, 0.99820846, 0.99947447, 0.9994506, 0.9994506, 0.99992836, 0.9992117, 0.9990923, 0.99906844, 0.9994267, 0.9999522, 0.9996417, 0.9980174, 0.998519, 0.99957, 0.9994506, 0.998949, 0.9996178, 0.99968946, 0.99992836, 0.99971336, 0.99844736, 0.9992834, 0.99949837, 0.9997611, 0.99990445, 0.99859065, 0.9991878, 0.9998328, 0.999785, 0.99971336, 0.9994506, 0.99985665, 0.99992836, 0.9981607, 0.998734, 0.9998328, 1.0, 0.9999522, 0.99899673]\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "back_smash  -->  fo_short         err_index number :  73\n",
      "back_smash  -->  fo_short         err_index number :  74\n",
      "back_smash  -->  fo_short         err_index number :  75\n",
      "back_smash  -->  fo_short         err_index number :  77\n",
      "back_smash  -->  fo_smash         err_index number :  84\n",
      "back_smash  -->  fo_short         err_index number :  85\n",
      "back_smash  -->  fo_short         err_index number :  86\n",
      "hit:  80  miss :  7 percent :  91.95402298850574\n"
     ]
    }
   ],
   "source": [
    "boot = np.array(boot)\n",
    "label = np.array(label)\n",
    "\n",
    "total_data = np.append(total_data,boot,axis=0)\n",
    "arr_label = np.append(arr_label,label,axis=0)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(total_data,arr_label,test_size=0.2,random_state=0)\n",
    "\n",
    "nsamples, nx, ny = x_train.shape\n",
    "d2_train_dataset = x_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = x_test.shape\n",
    "d2_test_dataset = x_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "session = tf.Session(config=config)\n",
    "\n",
    "\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model = Sequential() # Sequeatial Model \n",
    "    model.add(LSTM(180, input_shape=(60,6),return_sequences = True)) # (timestep, feature) \n",
    "    model.add(Dropout(0.8))\n",
    "    model.add(Conv1D(512,\n",
    "                     2,\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "    # 3. 모델 학습과정 설정하기\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    hist = model.fit(x_train, y_train, epochs=100, batch_size=64, validation_data=(x_test, y_test))\n",
    "    model.save('model.h5')\n",
    "    model.save_weights('model_weights.h5')\n",
    "print(\"Best: \" ,(hist.history['acc']))\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp.append(df['GX'][i]/1000)\n",
    "        tmp.append(df['GY'][i]/1000)\n",
    "        tmp.append(df['GZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "#sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = model.predict(total_data)\n",
    "sample_pred = np.argmax(sample_pred,axis=-1)\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'        err_index number : ',x)\n",
    "\n",
    "        \n",
    "print('hit: ',hit,' miss : ',miss,'percent : ',(100*hit)/(hit+miss))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_parameter is change :  {'kernel': 'linear', 'gamma': 0.001, 'C': 0.001}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.001}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.001}\n",
      "best_parameter is change :  {'kernel': 'linear', 'gamma': 0.001, 'C': 0.01}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.01}\n",
      "remain : {'kernel': 'linear', 'gamma': 0.001, 'C': 0.01}\n",
      "best_parameter is change :  {'kernel': 'linear', 'gamma': 0.001, 'C': 0.1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a0553a41afd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkernel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rbf'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'poly'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtmp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md2_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    392\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 232\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#####here to change####\n",
    "######################################################################\n",
    "\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "for gamma in [0.001,0.01,0.1,1,10,100]:\n",
    "    for C in [0.001,0.01,0.1,1,10,100]:\n",
    "        for kernel in ['linear','rbf','poly']:\n",
    "            tmp_model = svm.SVC(kernel=kernel,gamma=gamma,C=C)\n",
    "            scores = cross_val_score(tmp_model,d2_train_dataset,y_train,cv=10,n_jobs=-1)\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if score>best_score:\n",
    "                \n",
    "                best_score = score\n",
    "                best_parameter = {'kernel':kernel,'gamma':gamma,'C':C}\n",
    "                print('best_parameter is change : ',best_parameter)\n",
    "            else:\n",
    "                print('remain :',best_parameter)\n",
    "mod = svm.SVC(**best_parameter)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp.append(df['GX'][i]/1000)\n",
    "        tmp.append(df['GY'][i]/1000)\n",
    "        tmp.append(df['GZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####here to change####\n",
    "######################################################################\n",
    "\n",
    "space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "        'gamma': hp.uniform ('gamma', 1,9),\n",
    "        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "        'n_estimators': 180,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "mod =xgb.XGBClassifier(\n",
    "                    n_estimators =space['n_estimators'], max_depth = space['max_depth'], gamma = space['gamma'],\n",
    "                    reg_alpha = space['reg_alpha'],min_child_weight=space['min_child_weight'],\n",
    "                    colsample_bytree=space['colsample_bytree'])\n",
    "    \n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp.append(df['GX'][i]/1000)\n",
    "        tmp.append(df['GY'][i]/1000)\n",
    "        tmp.append(df['GZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##위에거는 xg boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####here to change####\n",
    "######################################################################\n",
    "\n",
    "\n",
    "mod = GaussianNB()\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp.append(df['GX'][i]/1000)\n",
    "        tmp.append(df['GY'][i]/1000)\n",
    "        tmp.append(df['GZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에거는 나이브 베이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####here to change####\n",
    "######################################################################\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "param_grid = { \n",
    "    'n_estimators': [10,15,20,30,40,50,100,200,300,500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [3,4,5,6,7,8,9,10,11,12],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "mod = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10,n_jobs=-1)\n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('model best parameter : ',predict_model.best_params_)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp.append(df['GX'][i]/1000)\n",
    "        tmp.append(df['GY'][i]/1000)\n",
    "        tmp.append(df['GZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에거는 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('parameter Searching....')\n",
    "#####here to change####\n",
    "######################################################################\n",
    "\n",
    "leaf_size = list(range(1,50))\n",
    "n_neighbors = list(range(1,8))\n",
    "p=[1,2]\n",
    "\n",
    "hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "mod = GridSearchCV(knn, hyperparameters, cv=10, n_jobs=-1)\n",
    "\n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "print(classification_report(prediction ,y_test))\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('test_data/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp.append(df['GX'][i]/1000)\n",
    "        tmp.append(df['GY'][i]/1000)\n",
    "        tmp.append(df['GZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
