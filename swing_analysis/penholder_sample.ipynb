{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this model is  RandomForest \n",
      "\n",
      "\n",
      "load_file_complete\n",
      "make DataFrame...\n",
      "make DataFrame complete ....\n",
      "label_encoding processing...\n",
      "model best parameter :  {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2', 'n_estimators': 300}\n",
      "fitting  RandomForest  is complete...\n",
      "RandomForest score is : 0.9767981438515081\n",
      "Validation Result...\n",
      "hit:  842  miss :  20\n",
      "\n",
      "\n",
      "now test new data..\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "fo_drive  -->  back_cut err_index :  0\n",
      "fo_drive  -->  back_cut err_index :  1\n",
      "fo_drive  -->  back_cut err_index :  4\n",
      "fo_drive  -->  back_cut err_index :  6\n",
      "fo_drive  -->  back_cut err_index :  7\n",
      "fo_drive  -->  back_cut err_index :  9\n",
      "fo_drive  -->  back_cut err_index :  10\n",
      "fo_drive  -->  back_cut err_index :  11\n",
      "fo_drive  -->  back_cut err_index :  12\n",
      "fo_drive  -->  back_cut err_index :  16\n",
      "fo_drive  -->  back_cut err_index :  17\n",
      "back_drive  -->  back_cut err_index :  18\n",
      "back_drive  -->  fo_smash err_index :  19\n",
      "back_drive  -->  fo_drive err_index :  20\n",
      "back_drive  -->  fo_smash err_index :  21\n",
      "back_drive  -->  fo_smash err_index :  22\n",
      "back_drive  -->  fo_smash err_index :  23\n",
      "back_drive  -->  back_cut err_index :  24\n",
      "back_drive  -->  fo_smash err_index :  25\n",
      "back_drive  -->  fo_smash err_index :  26\n",
      "back_drive  -->  fo_smash err_index :  27\n",
      "back_drive  -->  fo_drive err_index :  28\n",
      "back_drive  -->  fo_smash err_index :  29\n",
      "fo_smash  -->  fo_drive err_index :  30\n",
      "fo_smash  -->  fo_drive err_index :  31\n",
      "fo_smash  -->  back_smash err_index :  32\n",
      "fo_smash  -->  back_smash err_index :  33\n",
      "fo_smash  -->  fo_drive err_index :  34\n",
      "fo_smash  -->  fo_drive err_index :  35\n",
      "fo_smash  -->  fo_drive err_index :  36\n",
      "fo_smash  -->  fo_drive err_index :  37\n",
      "fo_smash  -->  fo_drive err_index :  39\n",
      "fo_smash  -->  fo_drive err_index :  41\n",
      "fo_smash  -->  back_smash err_index :  42\n",
      "fo_smash  -->  fo_drive err_index :  43\n",
      "fo_smash  -->  fo_drive err_index :  44\n",
      "fo_smash  -->  back_smash err_index :  46\n",
      "fo_short  -->  fo_drive err_index :  47\n",
      "fo_short  -->  fo_smash err_index :  48\n",
      "fo_short  -->  fo_smash err_index :  49\n",
      "fo_short  -->  fo_drive err_index :  50\n",
      "fo_short  -->  back_cut err_index :  51\n",
      "fo_short  -->  fo_smash err_index :  52\n",
      "fo_short  -->  fo_drive err_index :  53\n",
      "fo_short  -->  fo_drive err_index :  54\n",
      "fo_short  -->  fo_smash err_index :  55\n",
      "fo_short  -->  fo_smash err_index :  57\n",
      "fo_short  -->  back_smash err_index :  58\n",
      "fo_short  -->  fo_drive err_index :  59\n",
      "fo_short  -->  fo_drive err_index :  60\n",
      "fo_short  -->  fo_drive err_index :  61\n",
      "back_cut  -->  back_smash err_index :  62\n",
      "back_cut  -->  back_smash err_index :  63\n",
      "back_cut  -->  back_smash err_index :  64\n",
      "back_cut  -->  back_smash err_index :  65\n",
      "back_cut  -->  back_smash err_index :  66\n",
      "back_cut  -->  back_drive err_index :  67\n",
      "back_cut  -->  back_smash err_index :  68\n",
      "back_cut  -->  back_smash err_index :  69\n",
      "back_cut  -->  back_smash err_index :  70\n",
      "back_cut  -->  back_smash err_index :  71\n",
      "back_cut  -->  back_smash err_index :  72\n",
      "back_cut  -->  back_smash err_index :  73\n",
      "back_cut  -->  back_smash err_index :  74\n",
      "fo_cut  -->  back_smash err_index :  75\n",
      "fo_cut  -->  back_smash err_index :  76\n",
      "fo_cut  -->  back_smash err_index :  77\n",
      "fo_cut  -->  back_smash err_index :  78\n",
      "fo_cut  -->  back_smash err_index :  79\n",
      "fo_cut  -->  back_smash err_index :  80\n",
      "fo_cut  -->  back_smash err_index :  81\n",
      "fo_cut  -->  back_smash err_index :  82\n",
      "fo_cut  -->  back_drive err_index :  83\n",
      "fo_cut  -->  back_smash err_index :  84\n",
      "back_short  -->  back_cut err_index :  85\n",
      "back_short  -->  back_cut err_index :  86\n",
      "back_short  -->  fo_smash err_index :  87\n",
      "back_short  -->  fo_smash err_index :  88\n",
      "back_short  -->  fo_smash err_index :  89\n",
      "back_short  -->  fo_smash err_index :  90\n",
      "back_short  -->  fo_smash err_index :  91\n",
      "back_short  -->  fo_smash err_index :  92\n",
      "back_short  -->  fo_smash err_index :  93\n",
      "back_short  -->  fo_smash err_index :  94\n",
      "back_short  -->  fo_smash err_index :  95\n",
      "back_short  -->  fo_smash err_index :  96\n",
      "back_short  -->  fo_smash err_index :  97\n",
      "back_smash  -->  fo_smash err_index :  98\n",
      "back_smash  -->  fo_smash err_index :  100\n",
      "back_smash  -->  fo_smash err_index :  101\n",
      "back_smash  -->  fo_smash err_index :  102\n",
      "back_smash  -->  fo_smash err_index :  103\n",
      "back_smash  -->  fo_smash err_index :  104\n",
      "back_smash  -->  fo_smash err_index :  105\n",
      "back_smash  -->  fo_smash err_index :  106\n",
      "back_smash  -->  fo_smash err_index :  107\n",
      "back_smash  -->  fo_smash err_index :  108\n",
      "back_smash  -->  back_cut err_index :  109\n",
      "hit:  12  miss :  98\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.naive_bayes as nb\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import LSTM,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout,Input,Dense,Activation,Flatten,SeparableConv2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "target_folder = 'swing'\n",
    "mode = 'RandomForest'\n",
    "\n",
    "print('this model is ',mode,'\\n\\n')\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk(target_folder):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "print('load_file_complete')\n",
    "print('make DataFrame...')\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "print('make DataFrame complete ....')\n",
    "\n",
    "print('label_encoding processing...')\n",
    "encoder = LabelEncoder()\n",
    "arr_label = encoder.fit_transform(total_label)\n",
    "\n",
    "total_data = np.array(total_data)\n",
    "arr_label = np.array(arr_label)\n",
    "x_train,x_test,y_train,y_test = train_test_split(total_data,arr_label,test_size=0.2,random_state=0)\n",
    "\n",
    "nsamples, nx, ny = x_train.shape\n",
    "d2_train_dataset = x_train.reshape((nsamples,nx*ny))\n",
    "\n",
    "nsamples, nx, ny = x_test.shape\n",
    "d2_test_dataset = x_test.reshape((nsamples,nx*ny))\n",
    "\n",
    "\n",
    "#####here to change####\n",
    "######################################################################\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "param_grid = { \n",
    "    'n_estimators': [10,15,20,30,40,50,100,200,300,500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [3,4,5,6,7,8,9,10],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "mod = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10,n_jobs=-1)\n",
    "######################################################################\n",
    "\n",
    "\n",
    "predict_model = mod.fit(d2_train_dataset,y_train)\n",
    "print('model best parameter : ',predict_model.best_params_)\n",
    "print('fitting ',mode,' is complete...')\n",
    "print(mode,'score is :',predict_model.score(d2_test_dataset,y_test))\n",
    "\n",
    "prediction = predict_model.predict(d2_test_dataset)\n",
    "\n",
    "\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "for x in range(len(y_test)):\n",
    "    if prediction[x] == y_test[x]:\n",
    "        hit+=1\n",
    "    else:\n",
    "        miss+=1\n",
    "\n",
    "print('Validation Result...')\n",
    "print('hit: ',hit,' miss : ',miss)\n",
    "\n",
    "print('\\n\\nnow test new data..')\n",
    "\n",
    "filename_in_dir = []\n",
    "\n",
    "for root, dirs, files in os.walk('penholder/'):\n",
    "    for  fname in files:\n",
    "        full_fname = os.path.join(root, fname)\n",
    "        filename_in_dir.append(full_fname)\n",
    "\n",
    "total_data = []\n",
    "total_label = []\n",
    "\n",
    "for z in filename_in_dir:\n",
    "    sp = z.split('/')\n",
    "    label = sp[1]\n",
    "    f = open(z,'r')\n",
    "    d = f.read()\n",
    "    data = d.split('\\n')\n",
    "    data.pop(0)\n",
    "    index = data.pop(0)\n",
    "    real_data = []\n",
    "    for y in range(len(data)):\n",
    "        if data[y]=='':\n",
    "            continue\n",
    "        real_data.append(data[y].split(','))\n",
    "    df = pd.DataFrame(real_data)\n",
    "    index_li = index.split(',')\n",
    "    df.columns = index_li\n",
    "    for y in index_li:\n",
    "        df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "    tmp_li = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        tmp=[]\n",
    "        tmp.append(df['AX'][i]/1000)\n",
    "        tmp.append(df['AY'][i]/1000)\n",
    "        tmp.append(df['AZ'][i]/1000)\n",
    "        tmp_li.append(tmp)\n",
    "    \n",
    "    total_data.append(tmp_li)\n",
    "    total_label.append(label)\n",
    "    \n",
    "total_data = np.array(total_data)\n",
    "nsamples, nx, ny = total_data.shape\n",
    "sample = total_data.reshape((nsamples,nx*ny))\n",
    "\n",
    "sample_pred = predict_model.predict(sample)\n",
    "\n",
    "lab = encoder.inverse_transform(sample_pred)\n",
    "\n",
    "hit = 0\n",
    "miss = 0\n",
    "answer=[]\n",
    "print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "for x in range(len(lab)):\n",
    "    if lab[x] == total_label[x]:\n",
    "        hit+=1\n",
    "        answer.append(lab[x])\n",
    "    else:\n",
    "        miss+=1\n",
    "        print(total_label[x],' --> ' ,lab[x],'err_index : ',x)\n",
    "\n",
    "print('hit: ',hit,' miss : ',miss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
