{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/younghwan/.local/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.naive_bayes as nb\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import LSTM,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout,Input,Dense,Activation,Flatten,SeparableConv2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from sklearn.ensemble import AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load:\n",
    "    def __init__(self):\n",
    "        self.file_name_dir = []\n",
    "        self.total_data = []\n",
    "        self.total_label = []\n",
    "    \n",
    "    def load_file(self,dir_location):\n",
    "        print('now loading_file (location : ' + dir_location + ') ... \\n')\n",
    "        \n",
    "        for root,dirs,files in os.walk(dir_location):\n",
    "            for fname in files:\n",
    "                full_fname = os.path.join(root,fname)\n",
    "                self.file_name_dir.append(full_fname)\n",
    "        \n",
    "        print('make file list complete')\n",
    "    \n",
    "    def make_DataFrame(self,tar_li,p_n):\n",
    "        for file_name in tqdm(self.file_name_dir):\n",
    "            sp = file_name.split('/')\n",
    "            tmp_label = sp[1]\n",
    "            d = open(file_name,'r',encoding='UTF8').read()\n",
    "            data = d.split('\\n')\n",
    "            data.pop(0) # remove trash data header\n",
    "            index = data.pop(0)\n",
    "            tmp_real_data = []\n",
    "            for dat_num in range(len(data)):\n",
    "                if data[dat_num] == '':\n",
    "                    continue\n",
    "                tmp_real_data.append(data[dat_num].split(','))\n",
    "            \n",
    "            df = pd.DataFrame(tmp_real_data)\n",
    "            index_li = index.split(',')\n",
    "            df.columns = index_li\n",
    "            \n",
    "            #now change str to float\n",
    "            \n",
    "            for y in index_li:\n",
    "                df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "            \n",
    "            tmp_li = []\n",
    "            for i in range(len(df)):\n",
    "                tmp = []\n",
    "                for j in tar_li:\n",
    "                    tmp.append((df[j][i]/1000)**p_n)\n",
    "                tmp_li.append(tmp)\n",
    "            \n",
    "            self.total_data.append(tmp_li)\n",
    "            self.total_label.append(tmp_label)\n",
    "        print('make total_data finish.....')\n",
    "    \n",
    "    def return_data(self):\n",
    "        return self.total_data , self.total_label\n",
    "\n",
    "            \n",
    "\n",
    "         \n",
    "        \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.encoder = LabelEncoder()\n",
    "        self.enc_label = 0\n",
    "        \n",
    "        self.total_data = 0\n",
    "        self.total_label = 0\n",
    "        \n",
    "        self.x_train = 0\n",
    "        self.y_train = 0\n",
    "        self.x_test = 0\n",
    "        self.y_test = 0\n",
    "        \n",
    "        self.earlystopping = EarlyStopping(monitor='val_loss',patience=10)\n",
    "        \n",
    "        #model list\n",
    "        self.lstm = 0\n",
    "        self.svm = 0\n",
    "        self.xgboost = 0\n",
    "        self.nb = 0\n",
    "        self.rf =0\n",
    "        self.knn = 0\n",
    "        \n",
    "        #sample prediction\n",
    "        self.sample_data = 0\n",
    "        self.sample_label = 0\n",
    "        \n",
    "    def get_enc(self):\n",
    "        self.enc_label = self.encoder.fit_transform(self.total_label)\n",
    "    \n",
    "    def make_arr(self):\n",
    "        self.total_data = np.array(self.total_data)\n",
    "        self.enc_label =np.array(self.enc_label)\n",
    "    \n",
    "    def divide_dataset(self,mode):\n",
    "        \n",
    "        if mode == 'lstm':\n",
    "            self.x_train,self.x_test,self.y_train,self.y_test = train_test_split(self.total_data,self.enc_label,test_size=0.2,random_state=0)\n",
    "\n",
    "        else:\n",
    "            self.x_train,self.x_test,self.y_train,self.y_test = train_test_split(self.total_data,self.enc_label,test_size=0.2,random_state=0)\n",
    "            nsamples,nx,ny = self.x_train.shape\n",
    "            self.x_train = self.x_train.reshape((nsamples,nx*ny))\n",
    "            nsamples,nx,ny = self.x_test.shape\n",
    "            self.x_test = self.x_test.reshape((nsamples,nx*ny))\n",
    "        \n",
    "    def model_create_train(self,mode):\n",
    "        if mode == 'lstm':\n",
    "            with tf.device('/GPU:0'):\n",
    "                model = Sequential() # Sequeatial Model \n",
    "                model.add(LSTM(180, input_shape=(60,3),return_sequences = True)) # (timestep, feature) \n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Conv1D(128,\n",
    "                                 2,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu',\n",
    "                                 strides=1))\n",
    "                model.add(MaxPooling1D(pool_size=4))\n",
    "                model.add(LSTM(128))\n",
    "                model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "                # 3. 모델 학습과정 설정하기\n",
    "                model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "                hist = model.fit(self.x_train, self.y_train, epochs=100, batch_size=256,callbacks=[self.earlystopping] ,validation_data=(self.x_test, self.y_test))\n",
    "                model.save('model_x.h5')\n",
    "                model.save_weights('model_x_weights.h5')\n",
    "            self.lstm = model\n",
    "            \n",
    "        elif mode=='svm':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "\n",
    "            best_score = 0\n",
    "\n",
    "            for gamma in [0.001,0.01,0.1,1,10]:\n",
    "                for C in [0.001,0.01,0.1,1,10]:\n",
    "                    for kernel in ['linear','rbf','poly']:\n",
    "                        tmp_model = svm.SVC(kernel=kernel,gamma=gamma,C=C)\n",
    "                        scores = cross_val_score(tmp_model,self.x_train,self.y_train,cv=10,n_jobs=-1)\n",
    "                        score = np.mean(scores)\n",
    "\n",
    "                        if score>best_score:\n",
    "\n",
    "                            best_score = score\n",
    "                            best_parameter = {'kernel':kernel,'gamma':gamma,'C':C}\n",
    "                            print('best_parameter is change : ',best_parameter)\n",
    "                        else:\n",
    "                            print('remain :',best_parameter)\n",
    "            mod = svm.SVC(**best_parameter)\n",
    "            \n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.svm = mod\n",
    "        \n",
    "        elif mode=='xgboost':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "\n",
    "            space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "                    'gamma': hp.uniform ('gamma', 1,9),\n",
    "                    'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "                    'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "                    'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "                    'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "                    'n_estimators': 180,\n",
    "                    'seed': 0\n",
    "                }\n",
    "\n",
    "            mod =xgb.XGBClassifier(\n",
    "                                n_estimators =space['n_estimators'], max_depth = space['max_depth'], gamma = space['gamma'],\n",
    "                                reg_alpha = space['reg_alpha'],min_child_weight=space['min_child_weight'],\n",
    "                                colsample_bytree=space['colsample_bytree'])\n",
    "\n",
    "            \n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.xgboost = mod\n",
    "        \n",
    "        elif mode=='nb':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "\n",
    "            mod = GaussianNB()\n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.nb = mod\n",
    "        \n",
    "        \n",
    "        elif mode=='rf':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "            \n",
    "            rfc=RandomForestClassifier(random_state=42)\n",
    "            param_grid = { \n",
    "                'n_estimators': [10,15,20,30,40,50,100],\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                'max_depth' : [3,4,5,6,7,8,9],\n",
    "                'criterion' :['gini', 'entropy']\n",
    "            }\n",
    "\n",
    "            mod = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10,n_jobs=-1)\n",
    "\n",
    "            \n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.rf = mod\n",
    "        \n",
    "        elif mode=='knn':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "            \n",
    "            leaf_size = list(range(1,30))\n",
    "            n_neighbors = list(range(1,8))\n",
    "            p=[1,2]\n",
    "\n",
    "            hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "            knn = KNeighborsClassifier()\n",
    "\n",
    "            mod = GridSearchCV(knn, hyperparameters, cv=10, n_jobs=-1)\n",
    "\n",
    "            \n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.knn = mod\n",
    "        \n",
    "    \n",
    "    def prediction(self,input_axis,mode,p_n):\n",
    "        p = load()\n",
    "        p.load_file('test_data')\n",
    "        p.make_DataFrame(input_axis,p_n)\n",
    "        self.sample_data , self.sample_label = p.return_data()\n",
    "        \n",
    "        if mode == 'lstm':\n",
    "            self.sample_data = np.array(self.sample_data)\n",
    "            sample_pred = self.lstm.predict(self.sample_data)\n",
    "            sample_pred = np.argmax(sample_pred,axis=-1)\n",
    "            lab = self.encoder.inverse_transform(sample_pred)\n",
    "            \n",
    "            hit = 0\n",
    "            miss = 0\n",
    "            answer=[]\n",
    "            print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "            for x in range(len(lab)):\n",
    "                if lab[x] == self.sample_label[x]:\n",
    "                    hit+=1\n",
    "                    answer.append(lab[x])\n",
    "                else:\n",
    "                    miss+=1\n",
    "                    print(total_label[x],' --> ' ,lab[x],'        err_index number : ',x)\n",
    "\n",
    "\n",
    "            print('hit: ',hit,' miss : ',miss,'percent : ',(100*hit)/(hit+miss))\n",
    "        \n",
    "        else:\n",
    "            model_list = ['svm','knn','rf','nb','xgboost']\n",
    "            match_list = [self.svm , self.knn , self.rf , self.nb , self.xgboost]\n",
    "            \n",
    "            for x in range(len(model_list)):\n",
    "                if model_list[x] == mode:\n",
    "                    mod = match_list[x]\n",
    "                    print(mode + 'model match complete.....')\n",
    "                \n",
    "            self.sample_data = np.array(self.sample_data)\n",
    "            nsamples , nx , ny = self.sample_data.shape\n",
    "            sample = self.sample_data.reshape((nsamples,nx*ny))\n",
    "            \n",
    "            \n",
    "            sample_pred = mod.predict(sample)\n",
    "            lab = self.encoder.inverse_transform(sample_pred)\n",
    "            \n",
    "            hit = 0\n",
    "            miss = 0\n",
    "            answer=[]\n",
    "            print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "            for x in range(len(lab)):\n",
    "                if lab[x] == self.sample_label[x]:\n",
    "                    hit+=1\n",
    "                    answer.append(lab[x])\n",
    "                else:\n",
    "                    miss+=1\n",
    "                    print(total_label[x],' --> ' ,lab[x],'        err_index number : ',x)\n",
    "\n",
    "\n",
    "            print('hit: ',hit,' miss : ',miss,'percent : ',(100*hit)/(hit+miss))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def IO(tar_dir,input_axis,p_n):\n",
    "    total_dat = []\n",
    "    total_lab = []\n",
    "    v = load()\n",
    "    v.load_file(tar_dir)\n",
    "    v.make_DataFrame(input_axis,p_n)\n",
    "    total_dat,total_lab = v.return_data()\n",
    "    return total_dat , total_lab\n",
    "\n",
    "def pipline(total_data,total_label,input_axis,mode,p_n):\n",
    "    t = Train_model()\n",
    "    t.total_data = total_data\n",
    "    t.total_label = total_label\n",
    "    t.get_enc()\n",
    "    t.make_arr()\n",
    "    t.divide_dataset(mode)\n",
    "    t.model_create_train(mode)\n",
    "    t.prediction(input_axis,mode,p_n)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prac_machine(tar_dir,input_axis,mode_name,power):\n",
    "    total_data = []\n",
    "    total_label = []\n",
    "    print('Dir : '+tar_dir+'\\nthis ML model name is '+mode_name+'\\npower : '+str(power),'\\n\\n\\n')\n",
    "    total_data,total_label = IO(tar_dir,input_axis,power)\n",
    "    pipline(total_data,total_label,input_axis,mode_name,power)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/12331 [00:00<01:13, 167.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir : swing this ML model name is lstm power : 4\n",
      "now loading_file (location : swing) ... \n",
      "\n",
      "make file list complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12331/12331 [01:12<00:00, 169.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make total_data finish.....\n",
      "Train on 9864 samples, validate on 2467 samples\n",
      "Epoch 1/100\n",
      "9864/9864 [==============================] - 3s 329us/sample - loss: 0.9970 - acc: 0.6355 - val_loss: 0.4588 - val_acc: 0.8435\n",
      "Epoch 2/100\n",
      "9864/9864 [==============================] - 2s 231us/sample - loss: 0.3520 - acc: 0.8759 - val_loss: 0.2976 - val_acc: 0.8979\n",
      "Epoch 3/100\n",
      "9864/9864 [==============================] - 2s 229us/sample - loss: 0.2600 - acc: 0.9079 - val_loss: 0.2264 - val_acc: 0.9238\n",
      "Epoch 4/100\n",
      "9864/9864 [==============================] - 2s 233us/sample - loss: 0.2094 - acc: 0.9273 - val_loss: 0.2298 - val_acc: 0.9145\n",
      "Epoch 5/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.1826 - acc: 0.9377 - val_loss: 0.1671 - val_acc: 0.9441\n",
      "Epoch 6/100\n",
      "9864/9864 [==============================] - 2s 231us/sample - loss: 0.1463 - acc: 0.9503 - val_loss: 0.1550 - val_acc: 0.9493\n",
      "Epoch 7/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.1273 - acc: 0.9582 - val_loss: 0.1567 - val_acc: 0.9461\n",
      "Epoch 8/100\n",
      "9864/9864 [==============================] - 2s 229us/sample - loss: 0.1090 - acc: 0.9634 - val_loss: 0.1373 - val_acc: 0.9518\n",
      "Epoch 9/100\n",
      "9864/9864 [==============================] - 2s 229us/sample - loss: 0.0961 - acc: 0.9667 - val_loss: 0.1273 - val_acc: 0.9611\n",
      "Epoch 10/100\n",
      "9864/9864 [==============================] - 2s 233us/sample - loss: 0.0975 - acc: 0.9678 - val_loss: 0.1375 - val_acc: 0.9554\n",
      "Epoch 11/100\n",
      "9864/9864 [==============================] - 2s 233us/sample - loss: 0.0987 - acc: 0.9663 - val_loss: 0.1542 - val_acc: 0.9481\n",
      "Epoch 12/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.0749 - acc: 0.9766 - val_loss: 0.1165 - val_acc: 0.9619\n",
      "Epoch 13/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.0691 - acc: 0.9763 - val_loss: 0.1104 - val_acc: 0.9660\n",
      "Epoch 14/100\n",
      "9864/9864 [==============================] - 2s 229us/sample - loss: 0.0689 - acc: 0.9759 - val_loss: 0.1315 - val_acc: 0.9546\n",
      "Epoch 15/100\n",
      "9864/9864 [==============================] - 2s 229us/sample - loss: 0.0712 - acc: 0.9762 - val_loss: 0.1152 - val_acc: 0.9611\n",
      "Epoch 16/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.0521 - acc: 0.9830 - val_loss: 0.1130 - val_acc: 0.9631\n",
      "Epoch 17/100\n",
      "9864/9864 [==============================] - 2s 232us/sample - loss: 0.0480 - acc: 0.9843 - val_loss: 0.1042 - val_acc: 0.9664\n",
      "Epoch 18/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.0462 - acc: 0.9859 - val_loss: 0.1082 - val_acc: 0.9660\n",
      "Epoch 19/100\n",
      "9864/9864 [==============================] - 2s 231us/sample - loss: 0.0381 - acc: 0.9872 - val_loss: 0.1032 - val_acc: 0.9643\n",
      "Epoch 20/100\n",
      "9864/9864 [==============================] - 2s 229us/sample - loss: 0.0396 - acc: 0.9879 - val_loss: 0.1114 - val_acc: 0.9668\n",
      "Epoch 21/100\n",
      "9864/9864 [==============================] - 2s 229us/sample - loss: 0.0339 - acc: 0.9888 - val_loss: 0.1073 - val_acc: 0.9692\n",
      "Epoch 22/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.0343 - acc: 0.9898 - val_loss: 0.1026 - val_acc: 0.9712\n",
      "Epoch 23/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.0447 - acc: 0.9840 - val_loss: 0.1156 - val_acc: 0.9631\n",
      "Epoch 24/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.0420 - acc: 0.9859 - val_loss: 0.0965 - val_acc: 0.9688\n",
      "Epoch 25/100\n",
      "9864/9864 [==============================] - 2s 229us/sample - loss: 0.0368 - acc: 0.9876 - val_loss: 0.1022 - val_acc: 0.9704\n",
      "Epoch 26/100\n",
      "9864/9864 [==============================] - 2s 231us/sample - loss: 0.0251 - acc: 0.9930 - val_loss: 0.1130 - val_acc: 0.9660\n",
      "Epoch 27/100\n",
      "9864/9864 [==============================] - 2s 232us/sample - loss: 0.0221 - acc: 0.9936 - val_loss: 0.0927 - val_acc: 0.9749\n",
      "Epoch 28/100\n",
      "9864/9864 [==============================] - 2s 233us/sample - loss: 0.0212 - acc: 0.9929 - val_loss: 0.0948 - val_acc: 0.9724\n",
      "Epoch 29/100\n",
      "9864/9864 [==============================] - 2s 231us/sample - loss: 0.0147 - acc: 0.9953 - val_loss: 0.0978 - val_acc: 0.9737\n",
      "Epoch 30/100\n",
      "9864/9864 [==============================] - 2s 231us/sample - loss: 0.0250 - acc: 0.9926 - val_loss: 0.1293 - val_acc: 0.9603\n",
      "Epoch 31/100\n",
      "9864/9864 [==============================] - 2s 232us/sample - loss: 0.0274 - acc: 0.9906 - val_loss: 0.1020 - val_acc: 0.9700\n",
      "Epoch 32/100\n",
      "9864/9864 [==============================] - 2s 229us/sample - loss: 0.0254 - acc: 0.9925 - val_loss: 0.1149 - val_acc: 0.9651\n",
      "Epoch 33/100\n",
      "9864/9864 [==============================] - 2s 231us/sample - loss: 0.0237 - acc: 0.9920 - val_loss: 0.1202 - val_acc: 0.9627\n",
      "Epoch 34/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.0227 - acc: 0.9937 - val_loss: 0.1014 - val_acc: 0.9716\n",
      "Epoch 35/100\n",
      "9864/9864 [==============================] - 2s 230us/sample - loss: 0.0188 - acc: 0.9945 - val_loss: 0.0978 - val_acc: 0.9728\n",
      "Epoch 36/100\n",
      "9864/9864 [==============================] - 2s 232us/sample - loss: 0.0120 - acc: 0.9967 - val_loss: 0.1027 - val_acc: 0.9716\n",
      "Epoch 37/100\n",
      "9864/9864 [==============================] - 2s 232us/sample - loss: 0.0100 - acc: 0.9974 - val_loss: 0.1042 - val_acc: 0.9728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 17/87 [00:00<00:00, 167.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loading_file (location : test_data) ... \n",
      "\n",
      "make file list complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 173.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make total_data finish.....\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "fo_drive  -->  fo_smash         err_index number :  9\n",
      "fo_drive  -->  fo_cut         err_index number :  84\n",
      "hit:  85  miss :  2 percent :  97.70114942528735\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'swing'\n",
    "input_axis = ['AX','AY','AZ']\n",
    "mode_name = 'lstm'\n",
    "power = 4\n",
    "\n",
    "prac_machine(dir_name,input_axis,mode_name,power)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
