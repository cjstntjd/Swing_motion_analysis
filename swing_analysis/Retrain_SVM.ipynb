{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.naive_bayes as nb\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras.layers import LSTM,Conv1D,MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense \n",
    "import tensorflow.keras.backend as K \n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout,Input,Dense,Activation,Flatten,SeparableConv2D,BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from sklearn.utils import resample\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9 \n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class load:\n",
    "    def __init__(self):\n",
    "        self.file_name_dir = []\n",
    "        self.total_data = []\n",
    "        self.total_label = []\n",
    "    \n",
    "    def load_file(self,dir_location):\n",
    "        print('now loading_file (location : ' + dir_location + ') ... \\n')\n",
    "        \n",
    "        for root,dirs,files in os.walk(dir_location):\n",
    "            for fname in files:\n",
    "                full_fname = os.path.join(root,fname)\n",
    "                self.file_name_dir.append(full_fname)\n",
    "        \n",
    "        print('make file list complete')\n",
    "    \n",
    "    def make_DataFrame(self,tar_li,p_n):\n",
    "        for file_name in tqdm(self.file_name_dir):\n",
    "            sp = file_name.split('/')\n",
    "            tmp_label = sp[1]\n",
    "            d = open(file_name,'r',encoding='UTF8').read()\n",
    "            data = d.split('\\n')\n",
    "            data.pop(0) # remove trash data header\n",
    "            index = data.pop(0)\n",
    "            tmp_real_data = []\n",
    "            for dat_num in range(len(data)):\n",
    "                if data[dat_num] == '':\n",
    "                    continue\n",
    "                tmp_real_data.append(data[dat_num].split(','))\n",
    "            \n",
    "            df = pd.DataFrame(tmp_real_data)\n",
    "            index_li = index.split(',')\n",
    "            df.columns = index_li\n",
    "            \n",
    "            #now change str to float\n",
    "            \n",
    "            for y in index_li:\n",
    "                df[y] = pd.to_numeric(df[y],downcast='float')\n",
    "            \n",
    "            tmp_li = []\n",
    "            for i in range(len(df)):\n",
    "                tmp = []\n",
    "                for j in tar_li:\n",
    "                    tmp.append((df[j][i]/1000)**p_n)\n",
    "                tmp_li.append(tmp)\n",
    "            \n",
    "            self.total_data.append(tmp_li)\n",
    "            self.total_label.append(tmp_label)\n",
    "        print('make total_data finish.....')\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    def return_data(self):\n",
    "        return self.total_data , self.total_label\n",
    "\n",
    "class Train_model:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.encoder = LabelEncoder()\n",
    "        self.enc_label = 0\n",
    "        \n",
    "        self.total_data = 0\n",
    "        self.total_label = 0\n",
    "        \n",
    "        self.x_train = 0\n",
    "        self.y_train = 0\n",
    "        self.x_test = 0\n",
    "        self.y_test = 0\n",
    "        \n",
    "        self.earlystopping = EarlyStopping(monitor='val_loss',patience=10)\n",
    "        \n",
    "        #model list\n",
    "        self.lstm = 0\n",
    "        self.svm = 0\n",
    "        self.xgboost = 0\n",
    "        self.nb = 0\n",
    "        self.rf =0\n",
    "        self.knn = 0\n",
    "        \n",
    "        #sample prediction\n",
    "        self.sample_data = 0\n",
    "        self.sample_label = 0\n",
    "        \n",
    "    def get_enc(self):\n",
    "        self.enc_label = self.encoder.fit_transform(self.total_label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def make_arr(self):\n",
    "        self.total_data = np.array(self.total_data)\n",
    "        self.enc_label =np.array(self.enc_label)\n",
    "        \n",
    "    def Data_Augmentation(self,nu):\n",
    "        \n",
    "        #Data Augmentation is very optional Function\n",
    "        \n",
    "        tmp_li = [0]\n",
    "        for x in range(len(self.enc_label)-1):\n",
    "            if self.enc_label[x] != self.enc_label[x+1]:\n",
    "                tmp_li.append(x)\n",
    "        tmp_li.append(len(self.enc_label)-1)\n",
    "        print('Augmentation Data index is : ',tmp_li)\n",
    "        \n",
    "        div_data = []\n",
    "        div_label = []\n",
    "        for x in range(len(tmp_li)-1):\n",
    "            div_tmp =[]\n",
    "            div_la = []\n",
    "            for y in range(tmp_li[x]+1,tmp_li[x+1]+1):\n",
    "                div_tmp.append(self.total_data[y])\n",
    "                div_la.append(self.enc_label[y])\n",
    "            div_data.append(div_tmp)\n",
    "            div_label.append(div_la)\n",
    "        \n",
    "        \n",
    "        boot = []\n",
    "        label = []\n",
    "        t_li = [5,1,7,6,0,4,2,3]\n",
    "        for x in range(len(div_data)):\n",
    "            tmp = resample(div_data[x],replace=True,n_samples = nu,random_state=1)\n",
    "            boot+=tmp\n",
    "            label += list(t_li[x] for i in range(nu))\n",
    "        \n",
    "        boot = np.array(boot)\n",
    "        label = np.array(label)\n",
    "        \n",
    "        self.total_data = np.append(self.total_data,boot,axis=0)\n",
    "        self.enc_label = np.append(self.enc_label,label,axis=0)\n",
    "        \n",
    "    def divide_dataset(self,mode):\n",
    "        \n",
    "        if mode == 'lstm':\n",
    "            self.x_train,self.x_test,self.y_train,self.y_test = train_test_split(self.total_data,self.enc_label,test_size=0.2,random_state=0)\n",
    "\n",
    "        else:\n",
    "            self.x_train,self.x_test,self.y_train,self.y_test = train_test_split(self.total_data,self.enc_label,test_size=0.2,random_state=0)\n",
    "            nsamples,nx,ny = self.x_train.shape\n",
    "            self.x_train = self.x_train.reshape((nsamples,nx*ny))\n",
    "            nsamples,nx,ny = self.x_test.shape\n",
    "            self.x_test = self.x_test.reshape((nsamples,nx*ny))\n",
    "        \n",
    "    def model_create_train(self,mode):\n",
    "        if mode == 'lstm':\n",
    "            with tf.device('/GPU:0'):\n",
    "                model = Sequential() # Sequeatial Model \n",
    "                model.add(LSTM(180, input_shape=(60,3),return_sequences = True)) # (timestep, feature) \n",
    "                model.add(Dropout(0.2))\n",
    "                model.add(Conv1D(128,\n",
    "                                 2,\n",
    "                                 padding='valid',\n",
    "                                 activation='relu',\n",
    "                                 strides=1))\n",
    "                model.add(MaxPooling1D(pool_size=4))\n",
    "                model.add(LSTM(128))\n",
    "                model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "                # 3. 모델 학습과정 설정하기\n",
    "                model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "                hist = model.fit(self.x_train, self.y_train, epochs=100, batch_size=256,callbacks=[self.earlystopping] ,validation_data=(self.x_test, self.y_test))\n",
    "                model.save('model_x.h5')\n",
    "                model.save_weights('model_x_weights.h5')\n",
    "            self.lstm = model\n",
    "            \n",
    "        elif mode=='svm':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "\n",
    "            param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n",
    "              'kernel': ['rbf']}  \n",
    "  \n",
    "            grid = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3) \n",
    "\n",
    "            # fitting the model for grid search\n",
    "            with parallel_backend('threading'):\n",
    "                grid.fit(self.x_train, self.y_train) \n",
    "                print(grid.best_params_) \n",
    "\n",
    "            # print how our model looks after hyper-parameter tuning \n",
    "            print(grid.best_estimator_)\n",
    "            mod = grid\n",
    "            \n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.svm = mod\n",
    "            joblib.dump(mod,str(mode)+'_model.pkl')\n",
    "        elif mode=='xgboost':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "\n",
    "            space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n",
    "                    'gamma': hp.uniform ('gamma', 1,9),\n",
    "                    'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n",
    "                    'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n",
    "                    'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n",
    "                    'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n",
    "                    'n_estimators': 180,\n",
    "                    'seed': 0\n",
    "                }\n",
    "\n",
    "            mod =xgb.XGBClassifier(\n",
    "                                n_estimators =space['n_estimators'], max_depth = space['max_depth'], gamma = space['gamma'],\n",
    "                                reg_alpha = space['reg_alpha'],min_child_weight=space['min_child_weight'],\n",
    "                                colsample_bytree=space['colsample_bytree'])\n",
    "\n",
    "            \n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.xgboost = mod\n",
    "            joblib.dump(mod,str(mode)+'_model.pkl')\n",
    "        elif mode=='nb':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "\n",
    "            mod = MLPClassifier()\n",
    "\n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.nb = mod\n",
    "            joblib.dump(mod,str(mode)+'_model.pkl')\n",
    "        \n",
    "        elif mode=='rf':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "            \n",
    "            rfc=RandomForestClassifier(random_state=42)\n",
    "            param_grid = { \n",
    "                'n_estimators': [10,15,20,30,40,50,100],\n",
    "                'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                'max_depth' : [3,4,5,6,7,8,9],\n",
    "                'criterion' :['gini', 'entropy']\n",
    "            }\n",
    "\n",
    "            mod = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 10)\n",
    "\n",
    "            \n",
    "            ######################################################################\n",
    "            \n",
    "            predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.rf = mod\n",
    "            joblib.dump(mod,str(mode)+'_model.pkl')\n",
    "        elif mode=='knn':\n",
    "            #####here to change####\n",
    "            ######################################################################\n",
    "            \n",
    "            leaf_size = list(range(1,30))\n",
    "            n_neighbors = list(range(1,8))\n",
    "            p=[1,2]\n",
    "\n",
    "            hyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n",
    "\n",
    "            knn = KNeighborsClassifier()\n",
    "\n",
    "            mod = GridSearchCV(knn, hyperparameters, cv=10)\n",
    "\n",
    "            \n",
    "            ######################################################################\n",
    "            with parallel_backend('threading'):\n",
    "                predict_model = mod.fit(self.x_train,self.y_train)\n",
    "            print('fitting ',mode,' is complete...')\n",
    "            print(mode,'score is :',predict_model.score(self.x_test,self.y_test))\n",
    "\n",
    "            prediction = predict_model.predict(self.x_test)\n",
    "            self.knn = mod\n",
    "        \n",
    "            joblib.dump(mod,str(mode)+'_model.pkl')\n",
    "    def prediction(self,input_axis,mode,p_n):\n",
    "        p = load()\n",
    "        p.load_file('test_data')\n",
    "        p.make_DataFrame(input_axis,p_n)\n",
    "        self.sample_data , self.sample_label = p.return_data()\n",
    "        \n",
    "        if mode == 'lstm':\n",
    "            self.sample_data = np.array(self.sample_data)\n",
    "            print('here is :',self.sample_data[0])\n",
    "            sample_pred = self.lstm.predict(self.sample_data)\n",
    "            sample_pred = np.argmax(sample_pred,axis=-1)\n",
    "            lab = self.encoder.inverse_transform(sample_pred)\n",
    "            \n",
    "            hit = 0\n",
    "            miss = 0\n",
    "            answer=[]\n",
    "            print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "            for x in range(len(lab)):\n",
    "                if lab[x] == self.sample_label[x]:\n",
    "                    hit+=1\n",
    "                    answer.append(lab[x])\n",
    "                else:\n",
    "                    miss+=1\n",
    "                    print(self.sample_label[x],' --> ' ,lab[x],'        err_index number : ',x)\n",
    "\n",
    "\n",
    "            print('hit: ',hit,' miss : ',miss,'percent : ',(100*hit)/(hit+miss))\n",
    "        \n",
    "        else:\n",
    "            model_list = ['svm','knn','rf','nb','xgboost']\n",
    "            match_list = [self.svm , self.knn , self.rf , self.nb , self.xgboost]\n",
    "            \n",
    "            for x in range(len(model_list)):\n",
    "                if model_list[x] == mode:\n",
    "                    mod = match_list[x]\n",
    "                    print(mode + 'model match complete.....')\n",
    "                \n",
    "            self.sample_data = np.array(self.sample_data)\n",
    "            nsamples , nx , ny = self.sample_data.shape\n",
    "            sample = self.sample_data.reshape((nsamples,nx*ny))\n",
    "            print('here is :',sample[0])\n",
    "            sample_pred = mod.predict(sample)\n",
    "            lab = self.encoder.inverse_transform(sample_pred)\n",
    "            \n",
    "            hit = 0\n",
    "            miss = 0\n",
    "            answer=[]\n",
    "            print('testing new data result :\\n[answer]  -->  [predict err]')\n",
    "            for x in range(len(lab)):\n",
    "                if lab[x] == self.sample_label[x]:\n",
    "                    hit+=1\n",
    "                    answer.append(lab[x])\n",
    "                else:\n",
    "                    miss+=1\n",
    "                    print(self.sample_label[x],' --> ' ,lab[x],'        err_index number : ',x)\n",
    "\n",
    "\n",
    "            print('hit: ',hit,' miss : ',miss,'percent : ',(100*hit)/(hit+miss))\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IO(tar_dir,input_axis,p_n):\n",
    "    total_dat = []\n",
    "    total_lab = []\n",
    "    v = load()\n",
    "    v.load_file(tar_dir)\n",
    "    v.make_DataFrame(input_axis,p_n)\n",
    "    total_dat,total_lab = v.return_data()\n",
    "    return total_dat , total_lab\n",
    "\n",
    "def pipline(total_data,total_label,input_axis,mode,p_n,aug):\n",
    "    t = Train_model()\n",
    "    t.total_data = total_data\n",
    "    t.total_label = total_label\n",
    "    t.get_enc()\n",
    "    t.make_arr()\n",
    "    if aug != 0:\n",
    "        t.Data_Augmentation(5000)\n",
    "    t.divide_dataset(mode)\n",
    "    t.model_create_train(mode)\n",
    "    t.prediction(input_axis,mode,p_n)\n",
    "\n",
    "    \n",
    "def prac_machine(tar_dir,input_axis,mode_name,power,is_aug):\n",
    "    total_data = []\n",
    "    total_label = []\n",
    "    print('Dir : '+tar_dir+'\\nthis ML model name is '+mode_name+'\\npower : '+str(power),'\\n\\n\\n')\n",
    "    total_data,total_label = IO(tar_dir,input_axis,power)\n",
    "    pipline(total_data,total_label,input_axis,mode_name,power,is_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/12331 [00:00<01:11, 172.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir : swing\n",
      "this ML model name is nb\n",
      "power : 1 \n",
      "\n",
      "\n",
      "\n",
      "now loading_file (location : swing) ... \n",
      "\n",
      "make file list complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12331/12331 [01:10<00:00, 173.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make total_data finish.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 17/87 [00:00<00:00, 161.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting  nb  is complete...\n",
      "nb score is : 0.9744629104175111\n",
      "now loading_file (location : test_data) ... \n",
      "\n",
      "make file list complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 169.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make total_data finish.....\n",
      "nbmodel match complete.....\n",
      "here is : [ 0.452  0.262 -0.791  0.453  0.25  -0.811  0.393  0.247 -0.857  0.372\n",
      "  0.258 -0.895  0.36   0.272 -0.882  0.376  0.275 -0.946  0.384  0.259\n",
      " -1.015  0.397  0.23  -0.984  0.355  0.189 -0.969  0.275  0.177 -1.029\n",
      "  0.216  0.161 -1.076  0.246  0.153 -1.091  0.258  0.156 -1.057  0.224\n",
      "  0.16  -1.096  0.202  0.173 -1.107  0.212  0.244 -1.036  0.199  0.312\n",
      " -0.989  0.244  0.413 -0.971  0.32   0.447 -0.914  0.348  0.414 -0.87\n",
      "  0.421  0.195 -0.908  0.462 -0.093 -1.122  0.571 -0.636 -1.263  0.519\n",
      " -1.078 -1.053  0.631 -1.945 -0.694  0.616 -2.818 -0.853  0.683 -4.043\n",
      " -1.156  1.411 -4.546 -0.603  3.086 -4.264  0.326  3.361 -3.84   0.327\n",
      "  4.354 -3.639  0.247  5.494 -2.612  0.436  5.872 -2.543  0.353  6.143\n",
      " -2.598  0.662  5.864 -2.596  0.532  4.391 -2.774  0.828  3.746 -3.03\n",
      "  0.894  3.356 -3.147  0.952  3.13  -3.014  1.118  2.741 -2.769  1.163\n",
      "  2.117 -2.534  1.163  1.756 -2.386  1.161  1.651 -2.218  1.087  1.756\n",
      " -2.083  1.194  1.67  -1.878  1.16   1.453 -1.708  0.915  1.301 -1.541\n",
      "  0.613  1.165 -1.436  0.687  1.173 -1.342  0.644  1.314 -1.135  0.574\n",
      "  1.306 -0.973  0.473  1.26  -0.802  0.196  1.07  -0.645 -0.121  1.041\n",
      " -0.627 -0.136  1.032 -0.607 -0.101  1.028 -0.613 -0.09   0.997 -0.63\n",
      " -0.087  0.95  -0.655 -0.096  0.908 -0.635 -0.079  0.89  -0.667 -0.115]\n",
      "testing new data result :\n",
      "[answer]  -->  [predict err]\n",
      "back_smash  -->  back_drive         err_index number :  73\n",
      "back_smash  -->  back_cut         err_index number :  74\n",
      "back_smash  -->  back_drive         err_index number :  75\n",
      "back_smash  -->  fo_drive         err_index number :  77\n",
      "back_smash  -->  fo_drive         err_index number :  84\n",
      "back_smash  -->  back_cut         err_index number :  85\n",
      "back_smash  -->  back_drive         err_index number :  86\n",
      "hit:  80  miss :  7 percent :  91.95402298850574\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'swing'\n",
    "input_axis = ['AX','AY','AZ']\n",
    "mode_list = ['nb']\n",
    "power = 1\n",
    "is_aug = 0\n",
    "\n",
    "for mode_name in mode_list:\n",
    "    prac_machine(dir_name,input_axis,mode_name,power,is_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
